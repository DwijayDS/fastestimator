{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Getting Started\n",
    "\n",
    "## Overview\n",
    "Welcome to FastEstimator! In this tutorial we are going to cover:\n",
    "* [The three main APIs of FastEstimator: `Pipeline`, `Network`, `Estimator`](#t01ThreeMain)\n",
    "* [An image classification example](#t01ImageClassification)\n",
    "    * [Pipeline](#t01Pipeline)\n",
    "    * [Network](#t01Network)\n",
    "    * [Estimator](#t01Estimator)\n",
    "    * [Training](#t01Training)\n",
    "    * [Inferencing](#t01Inferencing)\n",
    "* [Related Apphub Examples](#t01Apphub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01ThreeMain'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three main APIs\n",
    "All deep learning training workï¬‚ows involve the following three essential components, each mapping to a critical API in FastEstimator.\n",
    "\n",
    "* **Data pipeline**: extracts data from disk/RAM, performs transformations. ->  `fe.Pipeline`\n",
    "\n",
    "\n",
    "* **Network**: performs trainable and differentiable operations. ->  `fe.Network`\n",
    "\n",
    "\n",
    "* **Training loop**: combines the data pipeline and network in an iterative process. ->  `fe.Estimator`\n",
    "\n",
    "<BR>\n",
    "<BR>\n",
    "Any deep learning task can be constructed by following the 3 main steps:\n",
    "<img src=\"../resources/t01_api.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01ImageClassification'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Pipeline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Pipeline\n",
    "We use FastEstimator dataset API to load the MNIST dataset. Please check out [tutorial 2](./t02_dataset.ipynb) for more details about the dataset API. In this case our data preprocessing involves: \n",
    "1. Expand image dimension from (28,28) to (28, 28, 1) for convenience during convolution operations.\n",
    "2. Rescale pixel values from [0, 255] to [0, 1].\n",
    "\n",
    "Please check out [tutorial 3](./t03_operator.ipynb) for details about `Operator` and [tutorial 4](./t04_pipeline.ipynb) for `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.dataset.data import mnist\n",
    "from fastestimator.op.numpyop.univariate import ExpandDims, Minmax\n",
    "\n",
    "train_data, eval_data = mnist.load_data()\n",
    "\n",
    "pipeline = fe.Pipeline(train_data=train_data,\n",
    "                       eval_data=eval_data,\n",
    "                       batch_size=32,\n",
    "                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Network'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Network\n",
    "\n",
    "The model definition can be either from `tf.keras.Model` or `torch.nn.Module`, for more info about network definitions, check out [tutorial 5](./t05_model.ipynb). The differentiable operations during training are listed as follows:\n",
    "\n",
    "1. Feed the preprocessed images to the network and get prediction scores.\n",
    "2. Calculate `CrossEntropy` (loss) between prediction scores and ground truth.\n",
    "3. Update the model by minimizing `CrossEntropy`.\n",
    "\n",
    "For more info about `Network` and its operators, check out [tutorial 6](./t06_network.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.architecture.tensorflow import LeNet\n",
    "# from fastestimator.architecture.pytorch import LeNet  # One can also use a pytorch model\n",
    "\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "\n",
    "model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n",
    "\n",
    "network = fe.Network(ops=[\n",
    "        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n",
    "        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n",
    "        UpdateOp(model=model, loss_name=\"ce\") \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Estimator'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Estimator\n",
    "We define the `Estimator` to connect the `Network` to the `Pipeline`, and compute accuracy as a validation metric. Please see [tutorial 7](./t07_estimator.ipynb) for more about `Estimator` and `Traces`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.trace.metric import Accuracy\n",
    "from fastestimator.trace.io import BestModelSaver\n",
    "import tempfile\n",
    "\n",
    "traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n",
    "          BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]\n",
    "\n",
    "estimator = fe.Estimator(pipeline=pipeline,\n",
    "                         network=network,\n",
    "                         epochs=2,\n",
    "                         traces=traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Training'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Start: step: 1; num_device: 0; logging_interval: 100; \n",
      "FastEstimator-Train: step: 1; ce: 2.325205; \n",
      "FastEstimator-Train: step: 100; ce: 0.37162033; steps/sec: 161.0; \n",
      "FastEstimator-Train: step: 200; ce: 0.24027318; steps/sec: 166.53; \n",
      "FastEstimator-Train: step: 300; ce: 0.042502172; steps/sec: 160.22; \n",
      "FastEstimator-Train: step: 400; ce: 0.08067161; steps/sec: 160.19; \n",
      "FastEstimator-Train: step: 500; ce: 0.0573852; steps/sec: 149.4; \n",
      "FastEstimator-Train: step: 600; ce: 0.0157291; steps/sec: 146.06; \n",
      "FastEstimator-Train: step: 700; ce: 0.21018827; steps/sec: 140.01; \n",
      "FastEstimator-Train: step: 800; ce: 0.008484628; steps/sec: 135.1; \n",
      "FastEstimator-Train: step: 900; ce: 0.02928259; steps/sec: 128.3; \n",
      "FastEstimator-Train: step: 1000; ce: 0.061196238; steps/sec: 126.4; \n",
      "FastEstimator-Train: step: 1100; ce: 0.06762987; steps/sec: 120.72; \n",
      "FastEstimator-Train: step: 1200; ce: 0.0072296523; steps/sec: 118.11; \n",
      "FastEstimator-Train: step: 1300; ce: 0.08244678; steps/sec: 110.16; \n",
      "FastEstimator-Train: step: 1400; ce: 0.07375234; steps/sec: 105.76; \n",
      "FastEstimator-Train: step: 1500; ce: 0.03207487; steps/sec: 104.01; \n",
      "FastEstimator-Train: step: 1600; ce: 0.1325811; steps/sec: 104.97; \n",
      "FastEstimator-Train: step: 1700; ce: 0.2333475; steps/sec: 99.21; \n",
      "FastEstimator-Train: step: 1800; ce: 0.081265345; steps/sec: 101.39; \n",
      "FastEstimator-Train: step: 1875; epoch: 1; epoch_time: 17.21 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmplq_y8tyg/model_best_accuracy.h5\n",
      "FastEstimator-Eval: step: 1875; epoch: 1; ce: 0.05035614; accuracy: 0.9828; since_best_accuracy: 0; max_accuracy: 0.9828; \n",
      "FastEstimator-Train: step: 1900; ce: 0.24747448; steps/sec: 100.72; \n",
      "FastEstimator-Train: step: 2000; ce: 0.056484234; steps/sec: 169.42; \n",
      "FastEstimator-Train: step: 2100; ce: 0.1583787; steps/sec: 186.35; \n",
      "FastEstimator-Train: step: 2200; ce: 0.004822081; steps/sec: 179.8; \n",
      "FastEstimator-Train: step: 2300; ce: 0.027388994; steps/sec: 180.22; \n",
      "FastEstimator-Train: step: 2400; ce: 0.017995346; steps/sec: 183.84; \n",
      "FastEstimator-Train: step: 2500; ce: 0.0071977032; steps/sec: 184.27; \n",
      "FastEstimator-Train: step: 2600; ce: 0.034278065; steps/sec: 182.51; \n",
      "FastEstimator-Train: step: 2700; ce: 0.045357186; steps/sec: 181.42; \n",
      "FastEstimator-Train: step: 2800; ce: 0.057187617; steps/sec: 182.88; \n",
      "FastEstimator-Train: step: 2900; ce: 0.04257428; steps/sec: 178.63; \n",
      "FastEstimator-Train: step: 3000; ce: 0.26984444; steps/sec: 167.96; \n",
      "FastEstimator-Train: step: 3100; ce: 0.026010124; steps/sec: 166.83; \n",
      "FastEstimator-Train: step: 3200; ce: 0.03834851; steps/sec: 161.82; \n",
      "FastEstimator-Train: step: 3300; ce: 0.01365272; steps/sec: 166.79; \n",
      "FastEstimator-Train: step: 3400; ce: 0.015053293; steps/sec: 164.75; \n",
      "FastEstimator-Train: step: 3500; ce: 0.0041770767; steps/sec: 163.45; \n",
      "FastEstimator-Train: step: 3600; ce: 0.0006832063; steps/sec: 162.57; \n",
      "FastEstimator-Train: step: 3700; ce: 0.015146113; steps/sec: 158.26; \n",
      "FastEstimator-Train: step: 3750; epoch: 2; epoch_time: 11.0 sec; \n",
      "FastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmplq_y8tyg/model_best_accuracy.h5\n",
      "FastEstimator-Eval: step: 3750; epoch: 2; ce: 0.0408412; accuracy: 0.9875; since_best_accuracy: 0; max_accuracy: 0.9875; \n",
      "FastEstimator-Finish: step: 3750; total_time: 30.16 sec; model_lr: 0.001; \n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Inferencing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing\n",
    "After training, we can do inferencing on new data with `Pipeline.transform` and `Netowork.transform`. Please checkout [tutorial 8](./t08_mode.ipynb) for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth class is 7\n",
      "Predicted class is 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 200x240 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = eval_data[0]\n",
    "data = pipeline.transform(data, mode=\"eval\")\n",
    "data = network.transform(data, mode=\"eval\")\n",
    "\n",
    "print(\"Ground truth class is {}\".format(data[\"y\"][0]))\n",
    "print(\"Predicted class is {}\".format(np.argmax(data[\"y_pred\"])))\n",
    "img = fe.util.ImgData(x=data[\"x\"])\n",
    "fig = img.paint_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Apphub'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apphub Examples\n",
    "You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:\n",
    "\n",
    "* [MNIST](../../apphub/image_classification/mnist/mnist.ipynb)\n",
    "* [DNN](../../apphub/tabular/dnn/dnn.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
