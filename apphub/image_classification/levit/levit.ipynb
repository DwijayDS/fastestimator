{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697eaca1",
   "metadata": {},
   "source": [
    "# Image Classification Using LeViT\n",
    "\n",
    "[[Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Graham_LeViT_A_Vision_Transformer_in_ConvNets_Clothing_for_Faster_Inference_ICCV_2021_paper.pdf)] [[Notebook](https://github.com/fastestimator/fastestimator/blob/master/apphub/image_classification/levit/levit.ipynb)] [[Torch Implementation](https://github.com/fastestimator/fastestimator/blob/master/apphub/image_classification/levit/levit_torch.py)][[Tensorflow Implementation](https://github.com/fastestimator/fastestimator/blob/master/apphub/image_classification/levit/levit_torch.py)]\n",
    "\n",
    "LeViT is a hybrid neural network for fast inference for image classification. LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. In this example, we will implement a LeViT in PyTorch and showcase how to use imagenet pre-trained a LeViT to fine-tune it on a downstream task for good results with minimal downstream training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa4f3c0",
   "metadata": {},
   "source": [
    "Let's first import some necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac4e0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 13:57:03.359029: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-16 13:57:04.129973: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-06-16 13:57:04.130114: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-06-16 13:57:04.130120: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "import fastestimator as fe\n",
    "from fastestimator.trace.io import BestModelSaver\n",
    "from fastestimator.dataset.data import cifair10\n",
    "from fastestimator.op.numpyop.meta import Sometimes\n",
    "from fastestimator.op.numpyop.multivariate import HorizontalFlip, Resize\n",
    "from fastestimator.op.numpyop.univariate import ChannelTranspose, CoarseDropout, Normalize, Onehot, Normalize\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.trace.adapt import LRScheduler\n",
    "from fastestimator.trace.metric import Accuracy\n",
    "from fastestimator.schedule import EpochScheduler, cosine_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e270c964",
   "metadata": {},
   "source": [
    "Now let's define some parameters that will be used later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da16074c",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "epochs=10\n",
    "data_dir=None\n",
    "train_steps_per_epoch=None\n",
    "eval_steps_per_epoch=None\n",
    "save_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a30cf8",
   "metadata": {},
   "source": [
    "Lets build the basic building blocks for the LeVIT model. We will be defining 3 levit models (LeViT_128S, LeViT_256, LeViT_384), where LeViT_128S is the smallest and LeVIT_384 is the largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b5e4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from\n",
    "# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "# Copyright 2020 Ross Wightman, Apache-2.0 License\n",
    "\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch.nn.init import trunc_normal_\n",
    "\n",
    "specification = {\n",
    "    'LeViT_128S': {\n",
    "        'embed_dim': [128, 256, 384],\n",
    "        'key_dim': 16,\n",
    "        'num_heads': [4, 6, 8],\n",
    "        'depth': [2, 3, 4],\n",
    "        'drop_path': 0,\n",
    "        'weights':\n",
    "        'https://huggingface.co/facebook/levit-128S/resolve/main/pytorch_model.bin'\n",
    "    },\n",
    "    'LeViT_256': {\n",
    "        'embed_dim': [256, 384, 512],\n",
    "        'key_dim': 32,\n",
    "        'num_heads': [4, 6, 8],\n",
    "        'depth': [4, 4, 4],\n",
    "        'drop_path': 0,\n",
    "        'weights':\n",
    "        'https://huggingface.co/facebook/levit-256/resolve/main/pytorch_model.bin'\n",
    "    },\n",
    "    'LeViT_384': {\n",
    "        'embed_dim': [384, 512, 768],\n",
    "        'key_dim': 32,\n",
    "        'num_heads': [6, 9, 12],\n",
    "        'depth': [4, 4, 4],\n",
    "        'drop_path': 0.1,\n",
    "        'weights':\n",
    "        'https://huggingface.co/facebook/levit-384/resolve/main/pytorch_model.bin'\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class ConvNorm(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=1,\n",
    "                 stride=1,\n",
    "                 padding=0,\n",
    "                 dilation=1,\n",
    "                 groups=1,\n",
    "                 bn_weight_init=1):\n",
    "        super().__init__()\n",
    "        self.convolution = torch.nn.Conv2d(in_channels,\n",
    "                                           out_channels,\n",
    "                                           kernel_size,\n",
    "                                           stride,\n",
    "                                           padding,\n",
    "                                           dilation,\n",
    "                                           groups,\n",
    "                                           bias=False)\n",
    "        self.batch_norm = torch.nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, bn_weight_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.batch_norm(self.convolution(x))\n",
    "\n",
    "\n",
    "class Backbone(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=2,\n",
    "                 padding=1):\n",
    "        super().__init__()\n",
    "        self.convolution_layer1 = ConvNorm(in_channels,\n",
    "                                           out_channels // 8,\n",
    "                                           kernel_size=kernel_size,\n",
    "                                           stride=stride,\n",
    "                                           padding=padding)\n",
    "        self.activation_layer1 = torch.nn.Hardswish()\n",
    "        self.convolution_layer2 = ConvNorm(out_channels // 8,\n",
    "                                           out_channels // 4,\n",
    "                                           kernel_size=kernel_size,\n",
    "                                           stride=stride,\n",
    "                                           padding=padding)\n",
    "        self.activation_layer2 = torch.nn.Hardswish()\n",
    "        self.convolution_layer3 = ConvNorm(out_channels // 4,\n",
    "                                           out_channels // 2,\n",
    "                                           kernel_size=kernel_size,\n",
    "                                           stride=stride,\n",
    "                                           padding=padding)\n",
    "        self.activation_layer3 = torch.nn.Hardswish()\n",
    "        self.convolution_layer4 = ConvNorm(out_channels // 2,\n",
    "                                           out_channels,\n",
    "                                           kernel_size=kernel_size,\n",
    "                                           stride=stride,\n",
    "                                           padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation_layer1(self.convolution_layer1(x))\n",
    "        x = self.activation_layer2(self.convolution_layer2(x))\n",
    "        x = self.activation_layer3(self.convolution_layer3(x))\n",
    "        return self.convolution_layer4(x)\n",
    "\n",
    "\n",
    "class LinearNorm(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bn_weight_init=1):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features, out_features, bias=False)\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(out_features)\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, bn_weight_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return self.batch_norm(x.flatten(0, 1)).reshape_as(x)\n",
    "\n",
    "\n",
    "class Downsample(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, stride, resolution, use_pool=False):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.resolution = resolution\n",
    "        self.pool = torch.nn.AvgPool2d(\n",
    "            3, stride=stride, padding=1,\n",
    "            count_include_pad=False) if use_pool else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, channels = x.shape\n",
    "        x = x.view(batch_size, self.resolution, self.resolution, channels)\n",
    "        if self.pool is not None:\n",
    "            x = self.pool(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
    "        else:\n",
    "            x = x[:, ::self.stride, ::self.stride]\n",
    "\n",
    "        return x.reshape(batch_size, -1, channels)\n",
    "\n",
    "\n",
    "class Residual(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, module, drop_rate):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.drop_out = torch.nn.Dropout(p=drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            return x + self.drop_out(self.module(x))\n",
    "        else:\n",
    "            return x + self.module(x)\n",
    "\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 key_dim,\n",
    "                 num_attention_heads=8,\n",
    "                 attention_ratio=4,\n",
    "                 resolution=14):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.scale = key_dim**-0.5\n",
    "        self.key_dim = key_dim\n",
    "        self.attention_ratio = attention_ratio\n",
    "\n",
    "        self.out_dim_keys_values = attention_ratio * key_dim * num_attention_heads + key_dim * num_attention_heads * 2\n",
    "        self.out_dim_projection = attention_ratio * key_dim * num_attention_heads\n",
    "\n",
    "        self.queries_keys_values = LinearNorm(dim, self.out_dim_keys_values)\n",
    "        self.activation = torch.nn.Hardswish()\n",
    "        self.projection = LinearNorm(self.out_dim_projection,\n",
    "                                     dim,\n",
    "                                     bn_weight_init=0)\n",
    "        points = list(itertools.product(range(resolution), range(resolution)))\n",
    "\n",
    "        len_points = len(points)\n",
    "        attention_offsets, indices = {}, []\n",
    "        for p1 in points:\n",
    "            for p2 in points:\n",
    "                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n",
    "                if offset not in attention_offsets:\n",
    "                    attention_offsets[offset] = len(attention_offsets)\n",
    "                indices.append(attention_offsets[offset])\n",
    "\n",
    "        self.attention_bias_cache = {}\n",
    "        self.attention_biases = torch.nn.Parameter(\n",
    "            torch.zeros(num_attention_heads, len(attention_offsets)))\n",
    "        self.register_buffer(\n",
    "            \"attention_bias_idxs\",\n",
    "            torch.LongTensor(indices).view(len_points, len_points))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        if mode and self.attention_bias_cache:\n",
    "            self.attention_bias_cache = {}  # clear ab cache\n",
    "\n",
    "    def get_attention_biases(self, device: torch.device) -> torch.Tensor:\n",
    "        if self.training:\n",
    "            return self.attention_biases[:, self.attention_bias_idxs]\n",
    "        else:\n",
    "            device_key = str(device)\n",
    "            if device_key not in self.attention_bias_cache:\n",
    "                self.attention_bias_cache[\n",
    "                    device_key] = self.attention_biases[:, self.\n",
    "                                                        attention_bias_idxs]\n",
    "            return self.attention_bias_cache[device_key]\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        batch_size, seq_length, _ = hidden_state.shape\n",
    "        queries_keys_values = self.queries_keys_values(hidden_state)\n",
    "        query, key, value = queries_keys_values.view(\n",
    "            batch_size, seq_length, self.num_attention_heads, -1).split([\n",
    "                self.key_dim, self.key_dim, self.attention_ratio * self.key_dim\n",
    "            ],\n",
    "                                                                        dim=3)\n",
    "        query = query.permute(0, 2, 1, 3)\n",
    "        key = key.permute(0, 2, 1, 3)\n",
    "        value = value.permute(0, 2, 1, 3)\n",
    "\n",
    "        attention = query @ key.transpose(\n",
    "            -2, -1) * self.scale + self.get_attention_biases(\n",
    "                hidden_state.device)\n",
    "        attention = attention.softmax(dim=-1)\n",
    "        hidden_state = (attention @ value).transpose(1, 2).reshape(\n",
    "            batch_size, seq_length, self.out_dim_projection)\n",
    "        hidden_state = self.projection(self.activation(hidden_state))\n",
    "        return hidden_state\n",
    "\n",
    "\n",
    "class AttentionDownsample(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        output_dim,\n",
    "        key_dim,\n",
    "        num_attention_heads,\n",
    "        attention_ratio,\n",
    "        stride,\n",
    "        resolution_in,\n",
    "        resolution_out,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.scale = key_dim**-0.5\n",
    "        self.key_dim = key_dim\n",
    "        self.attention_ratio = attention_ratio\n",
    "        self.out_dim_keys_values = attention_ratio * key_dim * num_attention_heads + key_dim * num_attention_heads\n",
    "        self.out_dim_projection = attention_ratio * key_dim * num_attention_heads\n",
    "        self.resolution_out = resolution_out\n",
    "        # resolution_in is the intial resolution, resoloution_out is final resolution after downsampling\n",
    "        self.keys_values = LinearNorm(input_dim, self.out_dim_keys_values)\n",
    "        self.queries_subsample = Downsample(stride, resolution_in)\n",
    "        self.queries = LinearNorm(input_dim, key_dim * num_attention_heads)\n",
    "        self.activation = torch.nn.Hardswish()\n",
    "        self.projection = LinearNorm(self.out_dim_projection, output_dim)\n",
    "\n",
    "        self.attention_bias_cache = {}\n",
    "\n",
    "        points = list(\n",
    "            itertools.product(range(resolution_in), range(resolution_in)))\n",
    "        points_ = list(\n",
    "            itertools.product(range(resolution_out), range(resolution_out)))\n",
    "        len_points, len_points_ = len(points), len(points_)\n",
    "        attention_offsets, indices = {}, []\n",
    "        for p1 in points_:\n",
    "            for p2 in points:\n",
    "                size = 1\n",
    "                offset = (abs(p1[0] * stride - p2[0] + (size - 1) / 2),\n",
    "                          abs(p1[1] * stride - p2[1] + (size - 1) / 2))\n",
    "                if offset not in attention_offsets:\n",
    "                    attention_offsets[offset] = len(attention_offsets)\n",
    "                indices.append(attention_offsets[offset])\n",
    "\n",
    "        self.attention_biases = torch.nn.Parameter(\n",
    "            torch.zeros(num_attention_heads, len(attention_offsets)))\n",
    "        self.register_buffer(\n",
    "            \"attention_bias_idxs\",\n",
    "            torch.LongTensor(indices).view(len_points_, len_points))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        if mode and self.attention_bias_cache:\n",
    "            self.attention_bias_cache = {}  # clear ab cache\n",
    "\n",
    "    def get_attention_biases(self, device):\n",
    "        if self.training:\n",
    "            return self.attention_biases[:, self.attention_bias_idxs]\n",
    "        else:\n",
    "            device_key = str(device)\n",
    "            if device_key not in self.attention_bias_cache:\n",
    "                self.attention_bias_cache[\n",
    "                    device_key] = self.attention_biases[:, self.\n",
    "                                                        attention_bias_idxs]\n",
    "            return self.attention_bias_cache[device_key]\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        batch_size, seq_length, _ = hidden_state.shape\n",
    "        key, value = (self.keys_values(hidden_state).view(\n",
    "            batch_size, seq_length, self.num_attention_heads,\n",
    "            -1).split([self.key_dim, self.attention_ratio * self.key_dim],\n",
    "                      dim=3))\n",
    "        key = key.permute(0, 2, 1, 3)\n",
    "        value = value.permute(0, 2, 1, 3)\n",
    "\n",
    "        query = self.queries(self.queries_subsample(hidden_state))\n",
    "        query = query.view(batch_size, self.resolution_out**2,\n",
    "                           self.num_attention_heads,\n",
    "                           self.key_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        attention = query @ key.transpose(\n",
    "            -2, -1) * self.scale + self.get_attention_biases(\n",
    "                hidden_state.device)\n",
    "        attention = attention.softmax(dim=-1)\n",
    "        hidden_state = (attention @ value).transpose(1, 2).reshape(\n",
    "            batch_size, -1, self.out_dim_projection)\n",
    "        hidden_state = self.projection(self.activation(hidden_state))\n",
    "        return hidden_state\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    MLP Layer with `2X` expansion in contrast to ViT with `4X`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.linear_up = LinearNorm(input_dim, hidden_dim)\n",
    "        self.activation = torch.nn.Hardswish()\n",
    "        self.linear_down = LinearNorm(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        hidden_state = self.linear_up(hidden_state)\n",
    "        hidden_state = self.activation(hidden_state)\n",
    "        hidden_state = self.linear_down(hidden_state)\n",
    "        return hidden_state\n",
    "\n",
    "\n",
    "class NormLinear(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 bias=True,\n",
    "                 std=0.02,\n",
    "                 drop=0.):\n",
    "        super().__init__()\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(in_features)\n",
    "        self.drop = torch.nn.Dropout(drop)\n",
    "        self.linear = torch.nn.Linear(in_features, out_features, bias=bias)\n",
    "        trunc_normal_(self.linear.weight, std=std)\n",
    "        if self.linear.bias is not None:\n",
    "            torch.nn.init.constant_(self.linear.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(self.drop(self.batch_norm(x)))\n",
    "\n",
    "\n",
    "class LeViT(torch.nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 patch_size=16,\n",
    "                 in_chans=3,\n",
    "                 num_classes=1000,\n",
    "                 embed_dim=[192],\n",
    "                 key_dim=[64],\n",
    "                 depth=[12],\n",
    "                 num_heads=[3],\n",
    "                 attention_ratio=[2],\n",
    "                 mlp_ratio=[2],\n",
    "                 down_ops=[],\n",
    "                 distillation=True,\n",
    "                 drop_path=0):\n",
    "        super().__init__()\n",
    "        resolution = img_size // patch_size\n",
    "        self.stages = []\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = embed_dim[-1]\n",
    "        self.embed_dim = embed_dim\n",
    "        self.distillation = distillation\n",
    "        self.patch_embed = Backbone(in_chans, embed_dim[0])\n",
    "\n",
    "        down_ops.append([''])\n",
    "\n",
    "        for i, (ed, kd, dpth, nh, ar, mr, do) in enumerate(\n",
    "                zip(embed_dim, key_dim, depth, num_heads, attention_ratio,\n",
    "                    mlp_ratio, down_ops)):\n",
    "            for _ in range(dpth):\n",
    "                self.stages.append(\n",
    "                    Residual(\n",
    "                        Attention(\n",
    "                            dim=ed,\n",
    "                            key_dim=kd,\n",
    "                            num_attention_heads=nh,\n",
    "                            attention_ratio=ar,\n",
    "                            resolution=resolution,\n",
    "                        ), drop_path))\n",
    "\n",
    "                if mr > 0:\n",
    "                    h = int(ed * mr)\n",
    "                    self.stages.append(\n",
    "                        Residual(MLP(input_dim=ed, hidden_dim=h), drop_path))\n",
    "\n",
    "            if do[0] == 'Subsample':\n",
    "                #('Subsample',key_dim, num_heads, attn_ratio, mlp_ratio, stride)\n",
    "                resolution_ = (resolution - 1) // do[5] + 1\n",
    "                self.stages.append(\n",
    "                    AttentionDownsample(input_dim=embed_dim[i],\n",
    "                                        output_dim=embed_dim[i + 1],\n",
    "                                        key_dim=do[1],\n",
    "                                        num_attention_heads=do[2],\n",
    "                                        attention_ratio=do[3],\n",
    "                                        stride=do[5],\n",
    "                                        resolution_in=resolution,\n",
    "                                        resolution_out=resolution_))\n",
    "                resolution = resolution_\n",
    "                if do[4] > 0:  # mlp_ratio\n",
    "                    h = int(embed_dim[i + 1] * do[4])\n",
    "                    self.stages.append(\n",
    "                        Residual(MLP(input_dim=embed_dim[i + 1], hidden_dim=h),\n",
    "                                 drop_path))\n",
    "\n",
    "        self.stages = torch.nn.Sequential(*self.stages)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = NormLinear(\n",
    "            embed_dim[-1],\n",
    "            num_classes) if num_classes > 0 else torch.nn.Identity()\n",
    "\n",
    "        if self.distillation:\n",
    "            self.head_dist = NormLinear(\n",
    "                embed_dim[-1],\n",
    "                num_classes) if num_classes > 0 else torch.nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.stages(x)\n",
    "        x = x.mean(1)\n",
    "\n",
    "        if self.distillation:\n",
    "            x = self.head(x), self.head_dist(x)\n",
    "            if not self.training:\n",
    "                x = (x[0] + x[1]) / 2\n",
    "        else:\n",
    "            x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def model_factory(embed_dim, key_dim, depth, num_heads, drop_path, weights,\n",
    "                  num_classes, distillation, pretrained):\n",
    "    model = LeViT(\n",
    "        patch_size=16,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        key_dim=[key_dim] * 3,\n",
    "        depth=depth,\n",
    "        attention_ratio=[2, 2, 2],\n",
    "        mlp_ratio=[2, 2, 2],\n",
    "        down_ops=[\n",
    "            #('Subsample',key_dim, num_heads, attn_ratio, mlp_ratio, stride)\n",
    "            ['Subsample', key_dim, embed_dim[0] // key_dim, 4, 2, 2],\n",
    "            ['Subsample', key_dim, embed_dim[1] // key_dim, 4, 2, 2],\n",
    "        ],\n",
    "        num_classes=num_classes,\n",
    "        drop_path=drop_path,\n",
    "        distillation=distillation)\n",
    "\n",
    "    if pretrained:\n",
    "        checkpoint_dict = torch.hub.load_state_dict_from_url(weights, map_location='cpu')\n",
    "        model_dict = model.state_dict()\n",
    "        model_keys = list(model_dict.keys())\n",
    "        checkpoint_keys = list(checkpoint_dict.keys())\n",
    "        for i, _ in enumerate(model_keys):\n",
    "            if not (model_keys[i].startswith('head.linear') or model_keys[i].startswith('head_dist.linear')):\n",
    "                model_dict[model_keys[i]] = checkpoint_dict[checkpoint_keys[i]]\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def LeViT_128S(num_classes=1000, distillation=False, pretrained=False):\n",
    "    return model_factory(**specification['LeViT_128S'],\n",
    "                         num_classes=num_classes,\n",
    "                         distillation=distillation,\n",
    "                         pretrained=pretrained)\n",
    "\n",
    "\n",
    "def LeViT_256(num_classes=1000, distillation=False, pretrained=False):\n",
    "    return model_factory(**specification['LeViT_256'],\n",
    "                         num_classes=num_classes,\n",
    "                         distillation=distillation,\n",
    "                         pretrained=pretrained)\n",
    "\n",
    "\n",
    "def LeViT_384(num_classes=1000, distillation=False, pretrained=False):\n",
    "    return model_factory(**specification['LeViT_384'],\n",
    "                         num_classes=num_classes,\n",
    "                         distillation=distillation,\n",
    "                         pretrained=pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec69df",
   "metadata": {},
   "source": [
    "Lets load cifair10 dataset and define the data argumentation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc8adb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, eval_data = cifair10.load_data(data_dir)\n",
    "\n",
    "pipeline = fe.Pipeline(\n",
    "    train_data=train_data,\n",
    "    eval_data=eval_data,\n",
    "    batch_size=batch_size,\n",
    "    ops=[\n",
    "        Resize(image_in=\"x\", image_out=\"x\", height=224, width=224),\n",
    "        Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n",
    "        Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),\n",
    "        CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),\n",
    "        ChannelTranspose(inputs=\"x\", outputs=\"x\"),\n",
    "        Onehot(inputs=\"y\", outputs=\"y\", mode=\"train\", num_classes=10, label_smoothing=0.05)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d5ed5",
   "metadata": {},
   "source": [
    "Lets define fastestimator model using the LeVIT-384. The downstream LeVIT is re-using the LeVIT encoder pre-trained on the imagenet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d854d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fe.build(model_fn=lambda: LeViT_384(num_classes=10, pretrained=True), optimizer_fn=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e290117",
   "metadata": {},
   "source": [
    "Now that the model is defined, lets create some network ops for optimizing the mode. We will be using the CrossEntropy as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6869a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = fe.Network(ops=[\n",
    "    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n",
    "    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", from_logits=True),\n",
    "    UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4bc909",
   "metadata": {},
   "source": [
    "Lets define some traces to save the model based on max accuracy and schedule the learning rate. Since learning rate warmup can help the transformer models to optimize faster lets define a learning rate scheduler, which will be slowly increase the learning rate for first 3 epochs, followed by using cosine_decay learning for every epoch after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a399a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 1e-2 / 64 * batch_size\n",
    "\n",
    "def lr_schedule_warmup(step, train_steps_epoch, init_lr):\n",
    "    warmup_steps = train_steps_epoch * 3\n",
    "    if step < warmup_steps:\n",
    "        lr = init_lr / warmup_steps * step\n",
    "    else:\n",
    "        lr = init_lr\n",
    "    return lr\n",
    "\n",
    "lr_schedule = {\n",
    "    1:\n",
    "    LRScheduler(\n",
    "        model=model,\n",
    "        lr_fn=lambda step: lr_schedule_warmup(\n",
    "            step, train_steps_epoch=np.ceil(len(train_data) / batch_size), init_lr=init_lr)),\n",
    "    4:\n",
    "    LRScheduler(\n",
    "        model=model,\n",
    "        lr_fn=lambda epoch: cosine_decay(\n",
    "            epoch, cycle_length=epochs - 3, init_lr=init_lr, min_lr=init_lr / 100, start=4))\n",
    "}\n",
    "\n",
    "traces = [\n",
    "    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n",
    "    BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\"),\n",
    "    EpochScheduler(lr_schedule)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d40addc",
   "metadata": {},
   "source": [
    "Finally its time for some actual training. To illustrate the effect of using the pre-trained encoder, we will only train the downstream task for a few epochs. So, lets define our estimator function and train for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e59a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = fe.Estimator(pipeline=pipeline,\n",
    "                         network=network,\n",
    "                         epochs=epochs,\n",
    "                         traces=traces,\n",
    "                         train_steps_per_epoch=train_steps_per_epoch,\n",
    "                         eval_steps_per_epoch=eval_steps_per_epoch,\n",
    "                         log_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f847cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mFastEstimator-Warn: Expected PyTorch version 2.0.1 but found 2.0.0+cu118. The framework may not work as expected.\u001b[00m\n",
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Start: step: 1; logging_interval: 100; num_device: 1;\n",
      "FastEstimator-Train: step: 1; ce: 2.327196; model_lr: 4.2625747e-06;\n",
      "FastEstimator-Train: step: 100; ce: 1.3989171; model_lr: 0.00042625747; steps/sec: 9.8;\n",
      "FastEstimator-Train: step: 200; ce: 1.0850445; model_lr: 0.00085251493; steps/sec: 9.69;\n",
      "FastEstimator-Train: step: 300; ce: 1.0998104; model_lr: 0.0012787724; steps/sec: 9.59;\n",
      "FastEstimator-Train: step: 400; ce: 1.1783917; model_lr: 0.0017050299; steps/sec: 9.08;\n",
      "FastEstimator-Train: step: 500; ce: 1.110758; model_lr: 0.0021312872; steps/sec: 9.23;\n",
      "FastEstimator-Train: step: 600; ce: 1.177232; model_lr: 0.0025575447; steps/sec: 9.3;\n",
      "FastEstimator-Train: step: 700; ce: 1.0568419; model_lr: 0.0029838022; steps/sec: 9.45;\n",
      "FastEstimator-Train: step: 782; epoch: 1; epoch_time(sec): 88.55;\n",
      "Eval Progress: 1/157;\n",
      "Eval Progress: 52/157; steps/sec: 22.87;\n",
      "Eval Progress: 104/157; steps/sec: 23.72;\n",
      "Eval Progress: 157/157; steps/sec: 20.34;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpc89r025k/model_best_accuracy.pt\n",
      "FastEstimator-Eval: step: 782; epoch: 1; accuracy: 0.767; ce: 0.6860145; max_accuracy: 0.767; since_best_accuracy: 0;\n",
      "FastEstimator-Train: step: 800; ce: 0.92758816; model_lr: 0.0034100597; steps/sec: 6.82;\n",
      "FastEstimator-Train: step: 900; ce: 0.8105689; model_lr: 0.0038363172; steps/sec: 8.88;\n",
      "FastEstimator-Train: step: 1000; ce: 0.84190565; model_lr: 0.0042625745; steps/sec: 8.84;\n",
      "FastEstimator-Train: step: 1100; ce: 0.6964846; model_lr: 0.004688832; steps/sec: 8.84;\n",
      "FastEstimator-Train: step: 1200; ce: 0.731653; model_lr: 0.0051150895; steps/sec: 8.86;\n",
      "FastEstimator-Train: step: 1300; ce: 0.6707432; model_lr: 0.005541347; steps/sec: 8.94;\n",
      "FastEstimator-Train: step: 1400; ce: 0.8042874; model_lr: 0.0059676045; steps/sec: 8.83;\n",
      "FastEstimator-Train: step: 1500; ce: 0.786237; model_lr: 0.006393862; steps/sec: 8.83;\n",
      "FastEstimator-Train: step: 1564; epoch: 2; epoch_time(sec): 92.35;\n",
      "Eval Progress: 1/157;\n",
      "Eval Progress: 52/157; steps/sec: 23.99;\n",
      "Eval Progress: 104/157; steps/sec: 24.63;\n",
      "Eval Progress: 157/157; steps/sec: 21.68;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpc89r025k/model_best_accuracy.pt\n",
      "FastEstimator-Eval: step: 1564; epoch: 2; accuracy: 0.783; ce: 0.65816414; max_accuracy: 0.783; since_best_accuracy: 0;\n",
      "FastEstimator-Train: step: 1600; ce: 0.67836124; model_lr: 0.0068201195; steps/sec: 6.52;\n",
      "FastEstimator-Train: step: 1700; ce: 0.54960984; model_lr: 0.007246377; steps/sec: 8.9;\n",
      "FastEstimator-Train: step: 1800; ce: 0.7021581; model_lr: 0.0076726344; steps/sec: 8.86;\n",
      "FastEstimator-Train: step: 1900; ce: 0.5233512; model_lr: 0.008098892; steps/sec: 8.77;\n",
      "FastEstimator-Train: step: 2000; ce: 0.72704124; model_lr: 0.008525149; steps/sec: 8.85;\n",
      "FastEstimator-Train: step: 2100; ce: 0.71746135; model_lr: 0.008951407; steps/sec: 8.85;\n",
      "FastEstimator-Train: step: 2200; ce: 0.5127801; model_lr: 0.009377664; steps/sec: 8.81;\n",
      "FastEstimator-Train: step: 2300; ce: 0.60386163; model_lr: 0.009803922; steps/sec: 8.86;\n",
      "FastEstimator-Train: step: 2346; epoch: 3; epoch_time(sec): 92.27;\n",
      "Eval Progress: 1/157;\n",
      "Eval Progress: 52/157; steps/sec: 23.25;\n",
      "Eval Progress: 104/157; steps/sec: 24.05;\n",
      "Eval Progress: 157/157; steps/sec: 22.08;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpc89r025k/model_best_accuracy.pt\n",
      "FastEstimator-Eval: step: 2346; epoch: 3; accuracy: 0.8477; ce: 0.4931664; max_accuracy: 0.8477; since_best_accuracy: 0;\n",
      "FastEstimator-Train: step: 2400; ce: 0.71735376; model_lr: 0.009509796; steps/sec: 6.66;\n",
      "FastEstimator-Train: step: 2500; ce: 0.67634785; model_lr: 0.009509796; steps/sec: 8.83;\n",
      "FastEstimator-Train: step: 2600; ce: 0.6946409; model_lr: 0.009509796; steps/sec: 8.64;\n",
      "FastEstimator-Train: step: 2700; ce: 0.6727053; model_lr: 0.009509796; steps/sec: 8.18;\n",
      "FastEstimator-Train: step: 2800; ce: 0.6282674; model_lr: 0.009509796; steps/sec: 6.82;\n",
      "FastEstimator-Train: step: 2900; ce: 0.39594457; model_lr: 0.009509796; steps/sec: 4.83;\n",
      "FastEstimator-Train: step: 3000; ce: 0.66740036; model_lr: 0.009509796; steps/sec: 4.86;\n",
      "FastEstimator-Train: step: 3100; ce: 0.7070837; model_lr: 0.009509796; steps/sec: 4.82;\n",
      "FastEstimator-Train: step: 3128; epoch: 4; epoch_time(sec): 127.42;\n",
      "Eval Progress: 1/157;\n",
      "Eval Progress: 52/157; steps/sec: 14.13;\n",
      "Eval Progress: 104/157; steps/sec: 14.74;\n",
      "Eval Progress: 157/157; steps/sec: 13.61;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpc89r025k/model_best_accuracy.pt\n",
      "FastEstimator-Eval: step: 3128; epoch: 4; accuracy: 0.8843; ce: 0.3831472; max_accuracy: 0.8843; since_best_accuracy: 0;\n",
      "FastEstimator-Train: step: 3200; ce: 0.54111826; model_lr: 0.008136274; steps/sec: 4.04;\n",
      "FastEstimator-Train: step: 3300; ce: 0.47495627; model_lr: 0.008136274; steps/sec: 4.84;\n",
      "FastEstimator-Train: step: 3400; ce: 0.55009687; model_lr: 0.008136274; steps/sec: 5.49;\n",
      "FastEstimator-Train: step: 3500; ce: 0.46195194; model_lr: 0.008136274; steps/sec: 6.45;\n",
      "FastEstimator-Train: step: 3600; ce: 0.54631186; model_lr: 0.008136274; steps/sec: 5.14;\n",
      "FastEstimator-Train: step: 3700; ce: 0.5319345; model_lr: 0.008136274; steps/sec: 5.36;\n",
      "FastEstimator-Train: step: 3800; ce: 0.4869312; model_lr: 0.008136274; steps/sec: 8.82;\n",
      "FastEstimator-Train: step: 3900; ce: 0.49249226; model_lr: 0.008136274; steps/sec: 8.69;\n",
      "FastEstimator-Train: step: 3910; epoch: 5; epoch_time(sec): 135.61;\n",
      "Eval Progress: 1/157;\n",
      "Eval Progress: 52/157; steps/sec: 23.03;\n",
      "Eval Progress: 104/157; steps/sec: 24.05;\n",
      "Eval Progress: 157/157; steps/sec: 22.79;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpc89r025k/model_best_accuracy.pt\n",
      "FastEstimator-Eval: step: 3910; epoch: 5; accuracy: 0.8915; ce: 0.35297596; max_accuracy: 0.8915; since_best_accuracy: 0;\n",
      "FastEstimator-Train: step: 4000; ce: 0.4870265; model_lr: 0.0061514787; steps/sec: 6.42;\n",
      "FastEstimator-Train: step: 4100; ce: 0.41959625; model_lr: 0.0061514787; steps/sec: 8.86;\n",
      "FastEstimator-Train: step: 4200; ce: 0.32927844; model_lr: 0.0061514787; steps/sec: 8.93;\n",
      "FastEstimator-Train: step: 4300; ce: 0.47794372; model_lr: 0.0061514787; steps/sec: 8.96;\n",
      "FastEstimator-Train: step: 4400; ce: 0.3883707; model_lr: 0.0061514787; steps/sec: 9.1;\n",
      "FastEstimator-Train: step: 4500; ce: 0.5392555; model_lr: 0.0061514787; steps/sec: 9.11;\n",
      "FastEstimator-Train: step: 4600; ce: 0.4398936; model_lr: 0.0061514787; steps/sec: 9.04;\n",
      "FastEstimator-Train: step: 4692; epoch: 6; epoch_time(sec): 90.98;\n",
      "Eval Progress: 1/157;\n",
      "Eval Progress: 52/157; steps/sec: 23.67;\n",
      "Eval Progress: 104/157; steps/sec: 24.7;\n",
      "Eval Progress: 157/157; steps/sec: 23.63;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpc89r025k/model_best_accuracy.pt\n",
      "FastEstimator-Eval: step: 4692; epoch: 6; accuracy: 0.8946; ce: 0.3373215; max_accuracy: 0.8946; since_best_accuracy: 0;\n",
      "FastEstimator-Train: step: 4700; ce: 0.44245452; model_lr: 0.0039485213; steps/sec: 6.69;\n",
      "FastEstimator-Train: step: 4800; ce: 0.32232776; model_lr: 0.0039485213; steps/sec: 9.02;\n",
      "FastEstimator-Train: step: 4900; ce: 0.44863296; model_lr: 0.0039485213; steps/sec: 9.02;\n",
      "FastEstimator-Train: step: 5000; ce: 0.42264745; model_lr: 0.0039485213; steps/sec: 9.01;\n",
      "FastEstimator-Train: step: 5100; ce: 0.60577977; model_lr: 0.0039485213; steps/sec: 8.88;\n",
      "FastEstimator-Train: step: 5200; ce: 0.5442467; model_lr: 0.0039485213; steps/sec: 9.03;\n",
      "FastEstimator-Train: step: 5300; ce: 0.42139134; model_lr: 0.0039485213; steps/sec: 8.98;\n",
      "FastEstimator-Train: step: 5400; ce: 0.5242033; model_lr: 0.0039485213; steps/sec: 8.94;\n",
      "FastEstimator-Train: step: 5474; epoch: 7; epoch_time(sec): 91.1;\n",
      "Eval Progress: 1/157;\n",
      "Eval Progress: 52/157; steps/sec: 24.21;\n",
      "Eval Progress: 104/157; steps/sec: 24.56;\n",
      "Eval Progress: 157/157; steps/sec: 22.17;\n",
      "FastEstimator-Eval: step: 5474; epoch: 7; accuracy: 0.8875; ce: 0.36601087; max_accuracy: 0.8946; since_best_accuracy: 1;\n",
      "FastEstimator-Train: step: 5500; ce: 0.43630207; model_lr: 0.0019637255; steps/sec: 6.71;\n",
      "FastEstimator-Train: step: 5600; ce: 0.33584255; model_lr: 0.0019637255; steps/sec: 9.0;\n",
      "FastEstimator-Train: step: 5700; ce: 0.5116489; model_lr: 0.0019637255; steps/sec: 8.99;\n",
      "FastEstimator-Train: step: 5800; ce: 0.4331568; model_lr: 0.0019637255; steps/sec: 9.02;\n",
      "FastEstimator-Train: step: 5900; ce: 0.49976832; model_lr: 0.0019637255; steps/sec: 8.99;\n",
      "FastEstimator-Train: step: 6000; ce: 0.33967438; model_lr: 0.0019637255; steps/sec: 9.1;\n",
      "FastEstimator-Train: step: 6100; ce: 0.34113187; model_lr: 0.0019637255; steps/sec: 9.11;\n",
      "FastEstimator-Train: step: 6200; ce: 0.38826856; model_lr: 0.0019637255; steps/sec: 9.04;\n",
      "FastEstimator-Train: step: 6256; epoch: 8; epoch_time(sec): 90.31;\n",
      "Eval Progress: 1/157;\n",
      "Eval Progress: 52/157; steps/sec: 23.94;\n",
      "Eval Progress: 104/157; steps/sec: 24.2;\n",
      "Eval Progress: 157/157; steps/sec: 22.05;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpc89r025k/model_best_accuracy.pt\n",
      "FastEstimator-Eval: step: 6256; epoch: 8; accuracy: 0.9125; ce: 0.2972072; max_accuracy: 0.9125; since_best_accuracy: 0;\n",
      "FastEstimator-Train: step: 6300; ce: 0.3560561; model_lr: 0.0005902041; steps/sec: 6.67;\n",
      "FastEstimator-Train: step: 6400; ce: 0.3423431; model_lr: 0.0005902041; steps/sec: 9.07;\n",
      "FastEstimator-Train: step: 6500; ce: 0.3707339; model_lr: 0.0005902041; steps/sec: 9.1;\n",
      "FastEstimator-Train: step: 6600; ce: 0.29217112; model_lr: 0.0005902041; steps/sec: 9.12;\n",
      "FastEstimator-Train: step: 6700; ce: 0.29924157; model_lr: 0.0005902041; steps/sec: 9.07;\n",
      "FastEstimator-Train: step: 6800; ce: 0.29773998; model_lr: 0.0005902041; steps/sec: 9.11;\n",
      "FastEstimator-Train: step: 6900; ce: 0.31682396; model_lr: 0.0005902041; steps/sec: 9.11;\n",
      "FastEstimator-Train: step: 7000; ce: 0.34464353; model_lr: 0.0005902041; steps/sec: 9.04;\n",
      "FastEstimator-Train: step: 7038; epoch: 9; epoch_time(sec): 90.02;\n",
      "Eval Progress: 1/157;\n",
      "Eval Progress: 52/157; steps/sec: 23.9;\n",
      "Eval Progress: 104/157; steps/sec: 24.43;\n",
      "Eval Progress: 157/157; steps/sec: 22.97;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpc89r025k/model_best_accuracy.pt\n",
      "FastEstimator-Eval: step: 7038; epoch: 9; accuracy: 0.9252; ce: 0.26771256; max_accuracy: 0.9252; since_best_accuracy: 0;\n",
      "FastEstimator-Train: step: 7100; ce: 0.28949457; model_lr: 1e-04; steps/sec: 6.69;\n",
      "FastEstimator-Train: step: 7200; ce: 0.28917986; model_lr: 1e-04; steps/sec: 8.98;\n",
      "FastEstimator-Train: step: 7300; ce: 0.3314595; model_lr: 1e-04; steps/sec: 9.04;\n",
      "FastEstimator-Train: step: 7400; ce: 0.29645276; model_lr: 1e-04; steps/sec: 9.03;\n",
      "FastEstimator-Train: step: 7500; ce: 0.35791695; model_lr: 1e-04; steps/sec: 8.99;\n",
      "FastEstimator-Train: step: 7600; ce: 0.28796023; model_lr: 1e-04; steps/sec: 9.08;\n",
      "FastEstimator-Train: step: 7700; ce: 0.37886822; model_lr: 1e-04; steps/sec: 9.07;\n",
      "FastEstimator-Train: step: 7800; ce: 0.29298776; model_lr: 1e-04; steps/sec: 9.04;\n",
      "FastEstimator-Train: step: 7820; epoch: 10; epoch_time(sec): 90.31;\n",
      "Eval Progress: 1/157;\n",
      "Eval Progress: 52/157; steps/sec: 24.07;\n",
      "Eval Progress: 104/157; steps/sec: 24.49;\n",
      "Eval Progress: 157/157; steps/sec: 22.13;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmpc89r025k/model_best_accuracy.pt\n",
      "FastEstimator-Eval: step: 7820; epoch: 10; accuracy: 0.9261; ce: 0.26642075; max_accuracy: 0.9261; since_best_accuracy: 0;\n",
      "FastEstimator-Finish: step: 7820; model_lr: 1e-04; total_time(sec): 1099.51;\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(warmup=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f735a67f",
   "metadata": {},
   "source": [
    "We are getting 92.6% accuracy which is not a bad results for 10 epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "73b4b63fab75275cc22557d8583617f6e244a28eab482e785f2d595a4709032f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
