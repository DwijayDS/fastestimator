{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 9: Inference\n",
    "## Overview\n",
    "In this tutorial we are going to cover:\n",
    "* [Running inference with the transform method](#t09inference)\n",
    "    * [Pipeline.transform](#t09pipeline)\n",
    "    * [Network.transform](#t09network)\n",
    "* [Related Apphub Examples](#t09apphub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t09inference'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference with transform method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running inference means using a trained deep learning model to get a prediction from some input data. Users can use `pipeline.transform` and `network.transform` to feed the data forward and get the computed result in any operation mode. Here we are going to use an end-to-end example (the same example code from [Tutorial 8](./t08_mode.ipynb)) on MNIST image classification to demonstrate how to run inference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first train a deep leaning model with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 20:38:28.358483: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-01 20:38:29.054427: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-11-01 20:38:29.054512: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-11-01 20:38:29.054518: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-11-01 20:38:33.588982: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-01 20:38:35.803998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38401 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:07:00.0, compute capability: 8.0\n",
      "2023-11-01 20:38:35.805534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38401 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0f:00.0, compute capability: 8.0\n",
      "2023-11-01 20:38:35.806848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38401 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:b7:00.0, compute capability: 8.0\n",
      "2023-11-01 20:38:35.808160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 38401 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:bd:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "\u001b[93mFastEstimator-Warn: Expected PyTorch version 2.0.1 but found 2.0.0+cu118. The framework may not work as expected.\u001b[00m\n",
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "\u001b[93mFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\u001b[00m\n",
      "\u001b[93mFastEstimator-Warn: The following key(s) are being pruned since they are unused outside of the Pipeline. To prevent this, you can declare the key(s) as inputs to Traces or TensorOps: x\u001b[00m\n",
      "FastEstimator-Start: step: 1; logging_interval: 100; num_device: 4;\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "FastEstimator-Train: step: 1; ce: 2.301598;\n",
      "FastEstimator-Train: step: 100; ce: 1.6592306; steps/sec: 84.3;\n",
      "FastEstimator-Train: step: 200; ce: 1.2675171; steps/sec: 104.71;\n",
      "FastEstimator-Train: step: 300; ce: 1.5682454; steps/sec: 100.33;\n",
      "FastEstimator-Train: step: 400; ce: 0.9771043; steps/sec: 106.77;\n",
      "FastEstimator-Train: step: 500; ce: 1.0616782; steps/sec: 100.53;\n",
      "FastEstimator-Train: step: 600; ce: 0.7245621; steps/sec: 104.54;\n",
      "FastEstimator-Train: step: 700; ce: 0.97258514; steps/sec: 107.73;\n",
      "FastEstimator-Train: step: 800; ce: 0.9991977; steps/sec: 97.39;\n",
      "FastEstimator-Train: step: 900; ce: 1.143329; steps/sec: 91.73;\n",
      "FastEstimator-Train: step: 1000; ce: 0.8395196; steps/sec: 95.2;\n",
      "FastEstimator-Train: step: 1100; ce: 1.0347806; steps/sec: 90.78;\n",
      "FastEstimator-Train: step: 1200; ce: 0.8687435; steps/sec: 91.46;\n",
      "FastEstimator-Train: step: 1300; ce: 1.0349537; steps/sec: 93.93;\n",
      "FastEstimator-Train: step: 1400; ce: 0.83892506; steps/sec: 101.11;\n",
      "FastEstimator-Train: step: 1500; ce: 0.8722749; steps/sec: 99.21;\n",
      "FastEstimator-Train: step: 1600; ce: 0.7330271; steps/sec: 92.48;\n",
      "FastEstimator-Train: step: 1700; ce: 0.88426745; steps/sec: 99.42;\n",
      "FastEstimator-Train: step: 1800; ce: 0.4187531; steps/sec: 103.03;\n",
      "FastEstimator-Train: step: 1875; epoch: 1; epoch_time(sec): 70.07;\n",
      "Eval Progress: 1/157;\n",
      "Eval Progress: 52/157; steps/sec: 105.69;\n",
      "Eval Progress: 104/157; steps/sec: 107.1;\n",
      "Eval Progress: 157/157; steps/sec: 29.2;\n",
      "FastEstimator-Eval: step: 1875; epoch: 1; accuracy: 0.9508; ce: 0.1691544;\n",
      "FastEstimator-Finish: step: 1875; model_lr: 0.001; total_time(sec): 117.64;\n"
     ]
    }
   ],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.dataset.data import mnist\n",
    "from fastestimator.trace.metric import Accuracy\n",
    "from fastestimator.op.numpyop.univariate import ExpandDims, Minmax, CoarseDropout\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "from fastestimator.architecture.tensorflow import LeNet\n",
    "\n",
    "\n",
    "train_data, eval_data = mnist.load_data()\n",
    "test_data = eval_data.split(0.5)\n",
    "model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n",
    "\n",
    "pipeline = fe.Pipeline(train_data=train_data,\n",
    "                       eval_data=eval_data,\n",
    "                       test_data=test_data,\n",
    "                       batch_size=32,\n",
    "                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), #default mode=None\n",
    "                            Minmax(inputs=\"x\", outputs=\"x_out\", mode=None),\n",
    "                            CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")])\n",
    "\n",
    "network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None\n",
    "                          CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),\n",
    "                          UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])\n",
    "\n",
    "estimator = fe.Estimator(pipeline=pipeline,\n",
    "                         network=network,\n",
    "                         epochs=1,\n",
    "                         traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\")) # default mode=[eval, test]\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a customized print function to showcase our inferencing easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "def print_dict_but_value(data):\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            print(\"{}: ndarray with shape {}\".format(key, value.shape))\n",
    "\n",
    "        elif isinstance(value, tf.Tensor):\n",
    "            print(\"{}: tf.Tensor with shape {}\".format(key, value.shape))\n",
    "\n",
    "        elif isinstance(value, torch.Tensor):\n",
    "            print(\"{}: torch Tensor with shape {}\".format(key, value.shape))\n",
    "\n",
    "        else:\n",
    "            print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows the complete execution graph (consisting `Pipeline` and `Network`) for the \"infer\" mode: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/fastestimator-util/fastestimator-misc/blob/master/resource/pictures/tutorial/t09_infer_mode.PNG?raw=true\" alt=\"drawing\" width=\"700\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to provide an input image \"x\" and get the prediction result \"y_pred\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t09pipeline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline.transform\n",
    "The `Pipeline` object has a `transform` method that runs the pipeline graph (\"x\" to \"x_out\") when inference data (a dictionary of keys and values like {\"x\":image}), is inserted. The returned output will be the dictionary of computed results after applying all `Pipeline` Ops, where the dictionary values will be Numpy arrays.\n",
    "<BR>\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/fastestimator-util/fastestimator-misc/blob/master/resource/pictures/tutorial/t09_infer_mode2.PNG?raw=true\" alt=\"drawing\"  width=\"700\"/>\n",
    "</p>\n",
    "<BR>\n",
    "Here we take eval_data's first image, package it into a dictionary, and then call `pipeline.transform`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: ndarray with shape (28, 28)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "infer_data = {\"x\": copy.deepcopy(eval_data[0][\"x\"])}\n",
    "print_dict_but_value(infer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: ndarray with shape (1, 28, 28, 1)\n",
      "x_out: ndarray with shape (1, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "infer_data = pipeline.transform(infer_data, mode=\"infer\")\n",
    "print_dict_but_value(infer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t09network'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network.transform\n",
    "\n",
    "We then use the network object to call the `transform` method that runs the network graph (\"x_out\" to \"y_pred\"). Much like with `pipeline.transform`, it will return it's Op outputs, though this time in the form of a dictionary of Tensors. The data type of the returned values depends on the backend of the network. It is `tf.Tensor` when using the TensorFlow backend and `torch.Tensor` with PyTorch. Please check out [Tutorial 6](./t06_network.ipynb) for more details about `Network` backends). \n",
    "\n",
    "<BR>\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/fastestimator-util/fastestimator-misc/blob/master/resource/pictures/tutorial/t09_infer_mode3.PNG?raw=true\" alt=\"drawing\" width=\"700\"/>\n",
    "</p>\n",
    "<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "x: tf.Tensor with shape (32, 28, 28, 1)\n",
      "x_out: tf.Tensor with shape (32, 28, 28, 1)\n",
      "y: tf.Tensor with shape (32,)\n",
      "y_pred: tf.Tensor with shape (32, 10)\n"
     ]
    }
   ],
   "source": [
    "infer_data = pipeline.get_results(mode=\"eval\") # you can also use the infer_data variable from above\n",
    "infer_data = network.transform(infer_data, mode=\"infer\")\n",
    "print_dict_but_value(infer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the input image and compare with its prediction class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Predicted class is {}\".format(np.argmax(infer_data[\"y_pred\"][0])))\n",
    "img = fe.util.BatchDisplay(image=infer_data[\"x\"][:1], title=\"x\")\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t09apphub'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apphub Examples\n",
    "You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:\n",
    "\n",
    "* [MNIST](../../apphub/image_classification/mnist/mnist.ipynb)\n",
    "* [IMDB](../../apphub/NLP/imdb/imdb.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
