{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "specification = {\n",
    "    'LeViT_128S': {\n",
    "        'embed_dim': [128, 256, 384],\n",
    "        'key_dim': 16,\n",
    "        'num_heads': [4, 6, 8],\n",
    "        'depth': [2, 3, 4],\n",
    "        'drop_path': 0,\n",
    "        'weights': 'https://huggingface.co/facebook/levit-128S/resolve/main/pytorch_model.bin'\n",
    "    },\n",
    "    'LeViT_256': {\n",
    "        'embed_dim': [256, 384, 512],\n",
    "        'key_dim': 32,\n",
    "        'num_heads': [4, 6, 8],\n",
    "        'depth': [4, 4, 4],\n",
    "        'drop_path': 0,\n",
    "        'weights': 'https://huggingface.co/facebook/levit-256/resolve/main/pytorch_model.bin'\n",
    "    },\n",
    "    'LeViT_384': {\n",
    "        'embed_dim': [384, 512, 768],\n",
    "        'key_dim': 32,\n",
    "        'num_heads': [6, 9, 12],\n",
    "        'depth': [4, 4, 4],\n",
    "        'drop_path': 0.1,\n",
    "        'weights': 'https://huggingface.co/facebook/levit-384/resolve/main/pytorch_model.bin'\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def hard_swish(features):\n",
    "    \"\"\"Computes a hard version of the swish function.\n",
    "\n",
    "    This operation can be used to reduce computational cost and improve\n",
    "    quantization for edge devices.\n",
    "\n",
    "    Args:\n",
    "        features: A `Tensor` representing preactivation values.\n",
    "\n",
    "    Returns:\n",
    "        The activation value.\n",
    "    \"\"\"\n",
    "    return features * tf.nn.relu6(features + tf.cast(3., features.dtype)) * (1. / 6.)\n",
    "\n",
    "\n",
    "class Backbone(layers.Layer):\n",
    "    def __init__(self, out_channels):\n",
    "        super(Backbone, self).__init__(name='backbone')\n",
    "        self.convolution_layer1 = layers.Conv2D(filters=out_channels // 8,\n",
    "                                                kernel_size=3,\n",
    "                                                strides=2,\n",
    "                                                padding=\"same\",\n",
    "                                                use_bias=False,\n",
    "                                                name='convolution_layer1')\n",
    "        self.batch_norm1 = layers.BatchNormalization(gamma_initializer='ones', name='batch_norm1')\n",
    "        self.convolution_layer2 = layers.Conv2D(filters=out_channels // 4,\n",
    "                                                kernel_size=3,\n",
    "                                                strides=2,\n",
    "                                                padding=\"same\",\n",
    "                                                use_bias=False,\n",
    "                                                name='convolution_layer2')\n",
    "        self.batch_norm2 = layers.BatchNormalization(gamma_initializer='ones', name='batch_norm2')\n",
    "        self.convolution_layer3 = layers.Conv2D(filters=out_channels // 2,\n",
    "                                                kernel_size=3,\n",
    "                                                strides=2,\n",
    "                                                padding=\"same\",\n",
    "                                                use_bias=False,\n",
    "                                                name='convolution_layer3')\n",
    "        self.batch_norm3 = layers.BatchNormalization(gamma_initializer='ones', name='batch_norm3')\n",
    "        self.convolution_layer4 = layers.Conv2D(filters=out_channels,\n",
    "                                                kernel_size=3,\n",
    "                                                strides=2,\n",
    "                                                padding=\"same\",\n",
    "                                                use_bias=False,\n",
    "                                                name='convolution_layer4')\n",
    "        self.batch_norm4 = layers.BatchNormalization(gamma_initializer='ones', name='batch_norm4')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = hard_swish(self.batch_norm1(self.convolution_layer1(x)))\n",
    "        x = hard_swish(self.batch_norm2(self.convolution_layer2(x)))\n",
    "        x = hard_swish(self.batch_norm3(self.convolution_layer3(x)))\n",
    "        x = hard_swish(self.batch_norm4(self.convolution_layer4(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Residual(layers.Layer):\n",
    "    def __init__(self, module, drop_rate=0., name='residual'):\n",
    "        super(Residual, self).__init__(name=name)\n",
    "        self.module = module\n",
    "        self.dropout = layers.Dropout(drop_rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        return x + self.dropout(self.module(x), training=training)\n",
    "\n",
    "\n",
    "class LinearNorm(layers.Layer):\n",
    "    def __init__(self, out_channels, bn_weight_init=1, name='linearnorm'):\n",
    "        super(LinearNorm, self).__init__(name=name)\n",
    "        self.batch_norm = layers.BatchNormalization(gamma_initializer=tf.constant_initializer(bn_weight_init))\n",
    "        self.linear = layers.Dense(out_channels, activation=None)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.linear(x)\n",
    "        shape = x.get_shape().as_list()\n",
    "        x = tf.reshape(self.batch_norm(tf.reshape(x, (-1, shape[2]))), shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Downsample(layers.Layer):\n",
    "    def __init__(self, stride, resolution, name='downsample'):\n",
    "        super(Downsample, self).__init__(name=name)\n",
    "        self.stride = stride\n",
    "        self.resolution = resolution\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size, _, channels = x.get_shape().as_list()\n",
    "        x = tf.reshape(x, (batch_size, self.resolution, self.resolution, channels))\n",
    "        x = x[:, ::self.stride, ::self.stride]\n",
    "        return tf.reshape(x, (batch_size, -1, channels))\n",
    "\n",
    "\n",
    "class NormLinear(layers.Layer):\n",
    "    def __init__(self, out_channels, bias=True, std=0.02, drop=0.0):\n",
    "        super(NormLinear, self).__init__(name='stem')\n",
    "        self.batch_norm = layers.BatchNormalization()\n",
    "        self.dropout = layers.Dropout(drop)\n",
    "        self.linear = layers.Dense(out_channels,\n",
    "                                   activation=None,\n",
    "                                   use_bias=bias,\n",
    "                                   kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0., stddev=std))\n",
    "\n",
    "    def call(self, x, training):\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(layers.Layer):\n",
    "    \"\"\"\n",
    "    MLP Layer with `2X` expansion in contrast to ViT with `4X`.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, name='mlp'):\n",
    "        super(MLP, self).__init__(name=name)\n",
    "        self.linear_up = LinearNorm(hidden_dim)\n",
    "        self.linear_down = LinearNorm(input_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.linear_down(hard_swish(self.linear_up(x)))\n",
    "\n",
    "\n",
    "class Attention(layers.Layer):\n",
    "    def __init__(self, input_dim, key_dim, num_attention_heads=8, attention_ratio=4, resolution=14, name='attention'):\n",
    "        super(Attention, self).__init__(name=name)\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.scale = key_dim**-0.5\n",
    "        self.key_dim = key_dim\n",
    "        self.attention_ratio = attention_ratio\n",
    "\n",
    "        self.out_dim_keys_values = attention_ratio * key_dim * num_attention_heads + key_dim * num_attention_heads * 2\n",
    "        self.out_dim_projection = attention_ratio * key_dim * num_attention_heads\n",
    "\n",
    "        self.queries_keys_values = LinearNorm(self.out_dim_keys_values)\n",
    "        self.projection = LinearNorm(input_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size, seq_length, _ = tf.shape(x)\n",
    "\n",
    "        queries_keys_values = self.queries_keys_values(x)\n",
    "\n",
    "        query, key, value = tf.split(tf.reshape(queries_keys_values, (batch_size, seq_length, self.num_attention_heads, -1)), [\n",
    "                self.key_dim, self.key_dim, self.attention_ratio * self.key_dim],axis=3)\n",
    "\n",
    "        query = tf.transpose(query, (0, 2, 1, 3))\n",
    "        key = tf.transpose(key, (0, 2, 1, 3))\n",
    "        value = tf.transpose(value, (0, 2, 1, 3))\n",
    "        attention = tf.matmul(query, key, transpose_b=True) * self.scale\n",
    "        attention = tf.nn.softmax(attention, axis=-1)\n",
    "        hidden_state = tf.reshape(tf.transpose(tf.matmul(attention, value), (0, 1, 3, 2)),\n",
    "                                  (batch_size, seq_length, self.out_dim_projection))\n",
    "        hidden_state = self.projection(hard_swish(hidden_state))\n",
    "        return hidden_state\n",
    "\n",
    "\n",
    "class AttentionDownsample(layers.Layer):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 key_dim,\n",
    "                 num_attention_heads,\n",
    "                 attention_ratio,\n",
    "                 stride,\n",
    "                 resolution_in,\n",
    "                 resolution_out,\n",
    "                 name='attention_downsample'):\n",
    "        super(AttentionDownsample, self).__init__(name=name)\n",
    "\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.scale = key_dim**-0.5\n",
    "        self.key_dim = key_dim\n",
    "        self.attention_ratio = attention_ratio\n",
    "        self.out_dim_keys_values = attention_ratio * key_dim * num_attention_heads + key_dim * num_attention_heads\n",
    "        self.out_dim_projection = attention_ratio * key_dim * num_attention_heads\n",
    "        self.resolution_out = resolution_out\n",
    "        self.resolution_in = resolution_in\n",
    "        # resolution_in is the intial resolution, resoloution_out is final resolution after downsampling\n",
    "        self.keys_values = LinearNorm(self.out_dim_keys_values)\n",
    "        self.queries_subsample = Downsample(stride, resolution_in)\n",
    "        self.queries = LinearNorm(key_dim * num_attention_heads)\n",
    "        self.projection = LinearNorm(output_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size, seq_length, _ = tf.shape(x)\n",
    "\n",
    "        key, value = tf.split(tf.reshape(self.keys_values(x), (\n",
    "            batch_size, seq_length, self.num_attention_heads,\n",
    "            -1)), [self.key_dim, self.attention_ratio * self.key_dim],\n",
    "                      axis=3)\n",
    "\n",
    "        key = tf.transpose(key, (0, 2, 1, 3))\n",
    "        value = tf.transpose(value, (0, 2, 1, 3))\n",
    "\n",
    "        query = self.queries(self.queries_subsample(x))\n",
    "        query = tf.transpose(\n",
    "            tf.reshape(query, (batch_size, self.resolution_out**2, self.num_attention_heads, self.key_dim)),\n",
    "            (0, 2, 1, 3))\n",
    "\n",
    "        attention = tf.matmul(query, key, transpose_b=True) * self.scale\n",
    "        attention = tf.nn.softmax(attention, axis=-1)\n",
    "        x = tf.reshape(tf.transpose(tf.matmul(attention, value), (0, 1, 3, 2)),\n",
    "                       (batch_size, -1, self.out_dim_projection))\n",
    "        x = self.projection(hard_swish(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def levit_stage(embed_dim,\n",
    "                key_dim,\n",
    "                num_attention_heads,\n",
    "                resolution,\n",
    "                depth,\n",
    "                attention_ratio,\n",
    "                mlp_ratio,\n",
    "                drop_path,\n",
    "                name='stage'):\n",
    "    stages = []\n",
    "    for i in range(depth):\n",
    "        stages.append(\n",
    "            Residual(\n",
    "                Attention(input_dim=embed_dim,\n",
    "                          key_dim=key_dim,\n",
    "                          num_attention_heads=num_attention_heads,\n",
    "                          attention_ratio=attention_ratio,\n",
    "                          resolution=resolution,\n",
    "                          name=name + '/attention' + str(i)),\n",
    "                drop_path,\n",
    "                name=name + '/attention' + str(i) + '/residual'))\n",
    "\n",
    "        if mlp_ratio > 0:\n",
    "            h = int(embed_dim * mlp_ratio)\n",
    "            stages.append(\n",
    "                Residual(MLP(input_dim=embed_dim, hidden_dim=h, name=name + '/mlp' + str(i)),\n",
    "                         drop_path,\n",
    "                         name=name + '/mlp' + str(i) + '/residual'))\n",
    "    return tf.keras.Sequential(stages, name=name)\n",
    "\n",
    "\n",
    "def levit_downsample(input_dim,\n",
    "                     output_dim,\n",
    "                     resolution,\n",
    "                     resolution_out,\n",
    "                     down_ops,\n",
    "                     drop_path,\n",
    "                     name='stage_downsample'):\n",
    "    stages = []\n",
    "    stages.append(\n",
    "        AttentionDownsample(input_dim=input_dim,\n",
    "                            output_dim=output_dim,\n",
    "                            key_dim=down_ops['key_dim'],\n",
    "                            num_attention_heads=down_ops['num_heads'],\n",
    "                            attention_ratio=down_ops['attn_ratio'],\n",
    "                            stride=down_ops['stride'],\n",
    "                            resolution_in=resolution,\n",
    "                            resolution_out=resolution_out,\n",
    "                            name=name + '/attention'))\n",
    "    if down_ops['mlp_ratio'] > 0:  # mlp_ratio\n",
    "        h = int(output_dim * down_ops['mlp_ratio'])\n",
    "        stages.append(\n",
    "            Residual(MLP(input_dim=output_dim, hidden_dim=h, name=name + '/mlp'), drop_path, name=name + '/residual'))\n",
    "    return tf.keras.Sequential(stages, name=name)\n",
    "\n",
    "class LeVIT(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 image_dim,\n",
    "                 patch_size,\n",
    "                 num_classes,\n",
    "                 embed_dim=[192],\n",
    "                 key_dim=[64],\n",
    "                 depth=[12],\n",
    "                 num_heads=[3],\n",
    "                 attention_ratio=[2],\n",
    "                 mlp_ratio=[2],\n",
    "                 down_ops={},\n",
    "                 distillation=True,\n",
    "                 drop_path=0.,\n",
    "                 name='Levit'):\n",
    "        super(LeVIT, self).__init__(name=name)\n",
    "        input_resolution_stage1 = image_dim // patch_size\n",
    "        input_resolution_stage2 = (input_resolution_stage1 - 1) // down_ops[1]['stride'] + 1\n",
    "        input_resolution_stage3 = (input_resolution_stage2 - 1) // down_ops[2]['stride'] + 1\n",
    "\n",
    "        self.backbone = Backbone(embed_dim[0])\n",
    "\n",
    "        self.stage1 = levit_stage(embed_dim=embed_dim[0],\n",
    "                        key_dim=key_dim[0],\n",
    "                        num_attention_heads=num_heads[0],\n",
    "                        resolution=input_resolution_stage1,\n",
    "                        depth=depth[0],\n",
    "                        attention_ratio=attention_ratio[0],\n",
    "                        mlp_ratio=mlp_ratio[0],\n",
    "                        drop_path=drop_path,\n",
    "                        name='stage1')\n",
    "\n",
    "        self.stage1_downsample = levit_downsample(input_dim=embed_dim[0],\n",
    "                                                  output_dim=embed_dim[1],\n",
    "                                                  resolution=input_resolution_stage1,\n",
    "                                                  resolution_out=input_resolution_stage2,\n",
    "                                                  down_ops=down_ops[1],\n",
    "                                                  drop_path=drop_path,\n",
    "                                                  name='stage1_downsample')\n",
    "\n",
    "        self.stage2 = levit_stage(embed_dim=embed_dim[1],\n",
    "                        key_dim=key_dim[1],\n",
    "                        num_attention_heads=num_heads[1],\n",
    "                        resolution=input_resolution_stage2,\n",
    "                        depth=depth[1],\n",
    "                        attention_ratio=attention_ratio[1],\n",
    "                        mlp_ratio=mlp_ratio[1],\n",
    "                        drop_path=drop_path,\n",
    "                        name='stage2')\n",
    "\n",
    "        self.stage2_downsample = levit_downsample(input_dim=embed_dim[1],\n",
    "                                                  output_dim=embed_dim[2],\n",
    "                                                  resolution=input_resolution_stage2,\n",
    "                                                  resolution_out=input_resolution_stage3,\n",
    "                                                  down_ops=down_ops[2],\n",
    "                                                  drop_path=drop_path,\n",
    "                                                  name='stage2_downsample')\n",
    "\n",
    "        self.stage3 = levit_stage(embed_dim=embed_dim[2],\n",
    "                                  key_dim=key_dim[2],\n",
    "                                  num_attention_heads=num_heads[2],\n",
    "                                  resolution=input_resolution_stage3,\n",
    "                                  depth=depth[2],\n",
    "                                  attention_ratio=attention_ratio[2],\n",
    "                                  mlp_ratio=mlp_ratio[2],\n",
    "                                  drop_path=drop_path,\n",
    "                                  name='stage3')\n",
    "\n",
    "        self.class_head = NormLinear(num_classes)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.backbone(x)\n",
    "        batch_size, _, _, channels = tf.shape(x)\n",
    "        x = tf.reshape(x, (batch_size, -1, channels))\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage1_downsample(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage2_downsample(x)\n",
    "        x = self.stage3(x)\n",
    "        x = tf.math.reduce_mean(x, -1)\n",
    "        x = self.class_head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def levit(input_shape,\n",
    "          patch_size,\n",
    "          num_classes,\n",
    "          embed_dim=[192],\n",
    "          key_dim=[64],\n",
    "          depth=[12],\n",
    "          num_heads=[3],\n",
    "          attention_ratio=[2],\n",
    "          mlp_ratio=[2],\n",
    "          down_ops={},\n",
    "          distillation=True,\n",
    "          drop_path=0.):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    input_resolution_stage1 = input_shape[0] // patch_size\n",
    "    input_resolution_stage2 = (input_resolution_stage1 - 1) // down_ops[1]['stride'] + 1\n",
    "    input_resolution_stage3 = (input_resolution_stage2 - 1) // down_ops[2]['stride'] + 1\n",
    "\n",
    "    x = Backbone(embed_dim[0])(inputs)\n",
    "    batch_size, input_resolution_stage1, input_resolution_stage1, channels = tf.shape(x)\n",
    "    x = tf.reshape(x, (batch_size, input_resolution_stage1 * input_resolution_stage1, channels))\n",
    "\n",
    "    x = levit_stage(embed_dim=embed_dim[0],\n",
    "                    key_dim=key_dim[0],\n",
    "                    num_attention_heads=num_heads[0],\n",
    "                    resolution=input_resolution_stage1,\n",
    "                    depth=depth[0],\n",
    "                    attention_ratio=attention_ratio[0],\n",
    "                    mlp_ratio=mlp_ratio[0],\n",
    "                    drop_path=drop_path,\n",
    "                    name='stage1')(x)\n",
    "\n",
    "    x = levit_downsample(input_dim=embed_dim[0],\n",
    "                         output_dim=embed_dim[1],\n",
    "                         resolution=input_resolution_stage1,\n",
    "                         resolution_out=input_resolution_stage2,\n",
    "                         down_ops=down_ops[1],\n",
    "                         drop_path=drop_path,\n",
    "                         name='stage1_downsample')(x)\n",
    "\n",
    "    x = levit_stage(embed_dim=embed_dim[1],\n",
    "                    key_dim=key_dim[1],\n",
    "                    num_attention_heads=num_heads[1],\n",
    "                    resolution=input_resolution_stage2,\n",
    "                    depth=depth[1],\n",
    "                    attention_ratio=attention_ratio[1],\n",
    "                    mlp_ratio=mlp_ratio[1],\n",
    "                    drop_path=drop_path,\n",
    "                    name='stage2')(x)\n",
    "\n",
    "    x = levit_downsample(input_dim=embed_dim[1],\n",
    "                         output_dim=embed_dim[2],\n",
    "                         resolution=input_resolution_stage2,\n",
    "                         resolution_out=input_resolution_stage3,\n",
    "                         down_ops=down_ops[2],\n",
    "                         drop_path=drop_path,\n",
    "                         name='stage2_downsample')(x)\n",
    "\n",
    "    x = levit_stage(embed_dim=embed_dim[2],\n",
    "                    key_dim=key_dim[2],\n",
    "                    num_attention_heads=num_heads[2],\n",
    "                    resolution=input_resolution_stage3,\n",
    "                    depth=depth[2],\n",
    "                    attention_ratio=attention_ratio[2],\n",
    "                    mlp_ratio=mlp_ratio[2],\n",
    "                    drop_path=drop_path,\n",
    "                    name='stage3')(x)\n",
    "\n",
    "    x = tf.math.reduce_mean(x, -1)\n",
    "    x = NormLinear(num_classes)(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "\n",
    "def LeViT_128S(image_dim, num_classes=1000, distillation=False, pretrained=False):\n",
    "    return model_factory(image_dim=image_dim,\n",
    "                         **specification['LeViT_128S'],\n",
    "                         num_classes=num_classes,\n",
    "                         distillation=distillation,\n",
    "                         pretrained=pretrained)\n",
    "\n",
    "\n",
    "def LeViT_256(image_dim, num_classes=1000, distillation=False, pretrained=False):\n",
    "    return model_factory(image_dim=image_dim,\n",
    "                         **specification['LeViT_256'],\n",
    "                         num_classes=num_classes,\n",
    "                         distillation=distillation,\n",
    "                         pretrained=pretrained)\n",
    "\n",
    "\n",
    "def LeViT_384(image_dim, num_classes=1000, distillation=False, pretrained=False):\n",
    "    return model_factory(image_dim=image_dim,\n",
    "                         **specification['LeViT_384'],\n",
    "                         num_classes=num_classes,\n",
    "                         distillation=distillation,\n",
    "                         pretrained=pretrained)\n",
    "\n",
    "\n",
    "def model_factory(image_dim,\n",
    "                  embed_dim,\n",
    "                  key_dim,\n",
    "                  depth,\n",
    "                  num_heads,\n",
    "                  drop_path,\n",
    "                  weights,\n",
    "                  num_classes,\n",
    "                  distillation,\n",
    "                  pretrained):\n",
    "    model = LeVIT(\n",
    "        image_dim,\n",
    "        patch_size=16,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        key_dim=[key_dim] * 3,\n",
    "        depth=depth,\n",
    "        attention_ratio=[2, 2, 2],\n",
    "        mlp_ratio=[2, 2, 2],\n",
    "        down_ops={\n",
    "            1: {\n",
    "                'key_dim': key_dim, 'num_heads': embed_dim[0] // key_dim, 'attn_ratio': 4, 'mlp_ratio': 2, 'stride': 2\n",
    "            },\n",
    "            2: {\n",
    "                'key_dim': key_dim, 'num_heads': embed_dim[1] // key_dim, 'attn_ratio': 4, 'mlp_ratio': 2, 'stride': 2\n",
    "            },\n",
    "        },\n",
    "        num_classes=num_classes,\n",
    "        drop_path=drop_path,\n",
    "        distillation=distillation)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from levit_torch import LeViT_128S, LeViT_256, LeViT_384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7010140\n",
      "17871982\n",
      "37596990\n"
     ]
    }
   ],
   "source": [
    "from levit_torch import LeViT_128S, LeViT_256, LeViT_384\n",
    "model_128 = LeViT_128S(num_classes=10)\n",
    "print(sum(p.numel() for p in model_128.parameters() if p.requires_grad))\n",
    "model_256 = LeViT_256(num_classes=10)\n",
    "print(sum(p.numel() for p in model_256.parameters() if p.requires_grad))\n",
    "model_384 = LeViT_384(num_classes=10)\n",
    "print(sum(p.numel() for p in model_384.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_biases = tf.Variable(tf.zeros((10, len([10, 20]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7019226\n",
      "17894122\n",
      "37628538\n"
     ]
    }
   ],
   "source": [
    "from levit_tf import LeViT_128S, LeViT_256, LeViT_384\n",
    "import numpy as np\n",
    "\n",
    "model_128 = LeViT_128S(image_dim=224, num_classes=10)\n",
    "model_128(tf.ones((4, 224, 224, 3)))\n",
    "print(np.sum([np.prod(v.get_shape().as_list()) for v in model_128.trainable_variables]))\n",
    "model_256 = LeViT_256(image_dim=224, num_classes=10)\n",
    "model_256(tf.ones((4, 224, 224, 3)))\n",
    "print(np.sum([np.prod(v.get_shape().as_list()) for v in model_256.trainable_variables]))\n",
    "model_384 = LeViT_384(image_dim=224, num_classes=10)\n",
    "model_384(tf.ones((4, 224, 224, 3)))\n",
    "print(np.sum([np.prod(v.get_shape().as_list()) for v in model_384.trainable_variables]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        model = load_bit_m_r50_1(framework=\"tf\")\n",
    "        self.assertEqual(, 23496256)\n",
    "\n",
    "    def test_bit_torch(self):\n",
    "        model = load_bit_m_r50_1(framework=\"torch\")\n",
    "        self.assertEqual(, 23496256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeViT_128S(image_dim=224, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function hard_swish at 0x000002265CF8F1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function hard_swish at 0x000002265CF8F1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "(4, 10)\n"
     ]
    }
   ],
   "source": [
    "output = model(tf.ones((4, 224, 224, 3)))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = [256, 384, 512]\n",
    "key_dim = [32, 32, 32]\n",
    "num_heads = [4, 6, 8]\n",
    "depth =  [4, 4, 4]\n",
    "drop_path =  0\n",
    "attention_ratio=[2, 2, 2]\n",
    "mlp_ratio=[2, 2, 2]\n",
    "down_ops=[\n",
    "    #('Subsample',key_dim, num_heads, attn_ratio, mlp_ratio, stride)\n",
    "    [32, embed_dim[0] // 32, 4, 2, 2],\n",
    "    [32, embed_dim[1] // 32, 4, 2, 2],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Backbone(embed_dim[0])(tf.ones((4, 224, 224, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, _, _ , channels = tf.shape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [224, 224, 3]\n",
    "patch_size = 16\n",
    "input_resolution_stage1 = input_shape[0] // patch_size\n",
    "input_resolution_stage2 = (input_resolution_stage1 - 1) // down_ops[0][4] + 1\n",
    "input_resolution_stage3 = (input_resolution_stage2 - 1) // down_ops[1][4] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\223044729\\Documents\\GitHub\\fastestimator\\apphub\\image_classification\\levit\\levit_test.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#Y131sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m Backbone(embed_dim[\u001b[39m0\u001b[39m])(tf\u001b[39m.\u001b[39;49mones((\u001b[39mNone\u001b[39;49;00m, \u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m, \u001b[39m3\u001b[39;49m)))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#Y131sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(x, (\u001b[39mNone\u001b[39;00m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m256\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#Y131sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m x \u001b[39m=\u001b[39m levit_stage(embed_dim\u001b[39m=\u001b[39membed_dim[\u001b[39m0\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#Y131sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                 key_dim\u001b[39m=\u001b[39mkey_dim[\u001b[39m0\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#Y131sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                 num_attention_heads\u001b[39m=\u001b[39mnum_heads[\u001b[39m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#Y131sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                 drop_path\u001b[39m=\u001b[39mdrop_path,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#Y131sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                 name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstage1\u001b[39m\u001b[39m'\u001b[39m)(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "x = Backbone(embed_dim[0])(tf.ones((None, 224, 224, 3)))\n",
    "x = tf.reshape(x, (None, -1, 256))\n",
    "x = levit_stage(embed_dim=embed_dim[0],\n",
    "                key_dim=key_dim[0],\n",
    "                num_attention_heads=num_heads[0],\n",
    "                resolution=input_resolution_stage1,\n",
    "                depth=depth[0],\n",
    "                attention_ratio=attention_ratio[0],\n",
    "                mlp_ratio=mlp_ratio[0],\n",
    "                drop_path=drop_path,\n",
    "                name='stage1')(x)\n",
    "\n",
    "x = levit_downsample(input_dim=embed_dim[0],\n",
    "                         output_dim=embed_dim[1],\n",
    "                         resolution=input_resolution_stage1,\n",
    "                         resolution_out=input_resolution_stage2,\n",
    "                         key_dim=down_ops[0][0],\n",
    "                         num_heads=down_ops[0][1],\n",
    "                         attn_ratio=down_ops[0][2],\n",
    "                         mlp_ratio=down_ops[0][3],\n",
    "                         stride=down_ops[0][4],\n",
    "                         drop_path=drop_path,\n",
    "                         name='stage1_downsample')(x)\n",
    "\n",
    "x = levit_stage(embed_dim=embed_dim[1], key_dim=key_dim[1], num_attention_heads=num_heads[1], resolution=input_resolution_stage2, depth=depth[1], attention_ratio=attention_ratio[1], mlp_ratio=mlp_ratio[1], drop_path=drop_path, name='stage2')(x)\n",
    "\n",
    "x = levit_downsample(input_dim=embed_dim[1],\n",
    "                        output_dim=embed_dim[2],\n",
    "                        resolution=input_resolution_stage2,\n",
    "                        resolution_out=input_resolution_stage3,\n",
    "                        key_dim=down_ops[1][0],\n",
    "                        num_heads=down_ops[1][1],\n",
    "                        attn_ratio=down_ops[1][2],\n",
    "                        mlp_ratio=down_ops[1][3],\n",
    "                        stride=down_ops[1][4],\n",
    "                        drop_path=drop_path,\n",
    "                        name='stage2_downsample')(x)\n",
    "\n",
    "x = levit_stage(embed_dim=embed_dim[2], key_dim=key_dim[2], num_attention_heads=num_heads[2], resolution=input_resolution_stage3, depth=depth[2], attention_ratio=attention_ratio[2], mlp_ratio=mlp_ratio[2], drop_path=drop_path, name='stage3')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16)\n"
     ]
    }
   ],
   "source": [
    "x = tf.math.reduce_mean(x, -1)\n",
    "print(x.shape)\n",
    "x = NormLinear(10)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 10])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_ratio 4\n",
      "key_dim 32\n",
      "num_attention_heads 8\n",
      "out_dim_keys_values 1280\n",
      "out_dim_projection 1024\n"
     ]
    }
   ],
   "source": [
    "downsample = AttentionDownsample(input_dim=embed_dim[0],\n",
    "                                 output_dim=embed_dim[1],\n",
    "                                 key_dim=down_ops[0][0],\n",
    "                                 num_attention_heads=down_ops[0][1],\n",
    "                                 attention_ratio=down_ops[0][2],\n",
    "                                 stride=down_ops[0][4],\n",
    "                                 resolution_in=input_resolution_stage1,\n",
    "                                 resolution_out=input_resolution_stage2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 196, 256])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([4, 4, 196, 32]),\n",
       " TensorShape([4, 4, 196, 32]),\n",
       " TensorShape([4, 4, 196, 64]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 196, 256])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'attention' (type Attention).\n\n{{function_node __wrapped__Transpose_device_/job:localhost/replica:0/task:0/device:CPU:0}} transpose expects a vector of size 4. But input(1) is a vector of size 2 [Op:Transpose]\n\nCall arguments received by layer 'attention' (type Attention):\n  • x=tf.Tensor(shape=(4, 196, 256), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\223044729\\Documents\\GitHub\\fastestimator\\apphub\\image_classification\\levit\\levit_test.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m att(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;32mc:\\Users\\223044729\\Documents\\GitHub\\fastestimator\\apphub\\image_classification\\levit\\levit_test.ipynb Cell 10\u001b[0m in \u001b[0;36mAttention.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#Y112sZmlsZQ%3D%3D?line=181'>182</a>\u001b[0m attention \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmatmul(query, key, transpose_b\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#Y112sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m attention \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msoftmax(attention, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#Y112sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m hidden_state \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(tf\u001b[39m.\u001b[39;49mtranspose(tf\u001b[39m.\u001b[39;49mmatmul(attention, value), (\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m)),\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#Y112sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m                           (batch_size, seq_length, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_dim_projection))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#Y112sZmlsZQ%3D%3D?line=185'>186</a>\u001b[0m hidden_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection(hard_swish(hidden_state))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#Y112sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_state\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'attention' (type Attention).\n\n{{function_node __wrapped__Transpose_device_/job:localhost/replica:0/task:0/device:CPU:0}} transpose expects a vector of size 4. But input(1) is a vector of size 2 [Op:Transpose]\n\nCall arguments received by layer 'attention' (type Attention):\n  • x=tf.Tensor(shape=(4, 196, 256), dtype=float32)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 14, 14, 256])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 196, 256), dtype=float32, numpy=\n",
       "array([[[-9.5882276e-03, -2.1390039e-03,  2.2422159e-03, ...,\n",
       "          6.2611694e-03,  8.9580305e-03,  2.3321765e-03],\n",
       "        [-9.5882276e-03, -2.1390039e-03,  2.2422159e-03, ...,\n",
       "          6.2611694e-03,  8.9580305e-03,  2.3321765e-03],\n",
       "        [-9.5882276e-03, -2.1390039e-03,  2.2422159e-03, ...,\n",
       "          6.2611694e-03,  8.9580305e-03,  2.3321765e-03],\n",
       "        ...,\n",
       "        [ 4.9301329e-05,  2.9083407e-03,  4.7255424e-03, ...,\n",
       "          3.0111913e-03,  5.1256054e-04, -4.8995493e-03],\n",
       "        [ 4.9301329e-05,  2.9083407e-03,  4.7255424e-03, ...,\n",
       "          3.0111913e-03,  5.1256054e-04, -4.8995493e-03],\n",
       "        [-2.9130569e-03, -2.9273028e-03,  1.0818355e-03, ...,\n",
       "         -2.9805668e-03,  4.4568088e-03, -9.3067074e-03]],\n",
       "\n",
       "       [[-9.5882276e-03, -2.1390039e-03,  2.2422159e-03, ...,\n",
       "          6.2611694e-03,  8.9580305e-03,  2.3321765e-03],\n",
       "        [-9.5882276e-03, -2.1390039e-03,  2.2422159e-03, ...,\n",
       "          6.2611694e-03,  8.9580305e-03,  2.3321765e-03],\n",
       "        [-9.5882276e-03, -2.1390039e-03,  2.2422159e-03, ...,\n",
       "          6.2611694e-03,  8.9580305e-03,  2.3321765e-03],\n",
       "        ...,\n",
       "        [ 4.9301329e-05,  2.9083407e-03,  4.7255424e-03, ...,\n",
       "          3.0111913e-03,  5.1256054e-04, -4.8995493e-03],\n",
       "        [ 4.9301329e-05,  2.9083407e-03,  4.7255424e-03, ...,\n",
       "          3.0111913e-03,  5.1256054e-04, -4.8995493e-03],\n",
       "        [-2.9130569e-03, -2.9273028e-03,  1.0818355e-03, ...,\n",
       "         -2.9805668e-03,  4.4568088e-03, -9.3067074e-03]],\n",
       "\n",
       "       [[-9.5882276e-03, -2.1390039e-03,  2.2422159e-03, ...,\n",
       "          6.2611694e-03,  8.9580305e-03,  2.3321765e-03],\n",
       "        [-9.5882276e-03, -2.1390039e-03,  2.2422159e-03, ...,\n",
       "          6.2611694e-03,  8.9580305e-03,  2.3321765e-03],\n",
       "        [-9.5882276e-03, -2.1390039e-03,  2.2422159e-03, ...,\n",
       "          6.2611694e-03,  8.9580305e-03,  2.3321765e-03],\n",
       "        ...,\n",
       "        [ 4.9301329e-05,  2.9083407e-03,  4.7255424e-03, ...,\n",
       "          3.0111913e-03,  5.1256054e-04, -4.8995493e-03],\n",
       "        [ 4.9301329e-05,  2.9083407e-03,  4.7255424e-03, ...,\n",
       "          3.0111913e-03,  5.1256054e-04, -4.8995493e-03],\n",
       "        [-2.9130569e-03, -2.9273028e-03,  1.0818355e-03, ...,\n",
       "         -2.9805668e-03,  4.4568088e-03, -9.3067074e-03]],\n",
       "\n",
       "       [[-9.5882276e-03, -2.1390039e-03,  2.2422159e-03, ...,\n",
       "          6.2611694e-03,  8.9580305e-03,  2.3321765e-03],\n",
       "        [-9.5882276e-03, -2.1390039e-03,  2.2422159e-03, ...,\n",
       "          6.2611694e-03,  8.9580305e-03,  2.3321765e-03],\n",
       "        [-9.5882276e-03, -2.1390039e-03,  2.2422159e-03, ...,\n",
       "          6.2611694e-03,  8.9580305e-03,  2.3321765e-03],\n",
       "        ...,\n",
       "        [ 4.9301329e-05,  2.9083407e-03,  4.7255424e-03, ...,\n",
       "          3.0111913e-03,  5.1256054e-04, -4.8995493e-03],\n",
       "        [ 4.9301329e-05,  2.9083407e-03,  4.7255424e-03, ...,\n",
       "          3.0111913e-03,  5.1256054e-04, -4.8995493e-03],\n",
       "        [-2.9130569e-03, -2.9273028e-03,  1.0818355e-03, ...,\n",
       "         -2.9805668e-03,  4.4568088e-03, -9.3067074e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "x = levit_downsample(input_dim=embed_dim[0], output_dim=embed_dim[1], resolution=input_resolution_stage1, resolution_out=input_resolution_stage2, key_dim=down_ops[0][0], num_heads=down_ops[0][1], attn_ratio=down_ops[0][2], mlp_ratio=down_ops[0][3], stride=down_ops[0][4], drop_path=drop_path)(x)\n",
    "\n",
    "x = levit_stage(embed_dim=embed_dim[1], key_dim=key_dim[1], num_attention_heads=num_heads[1], resolution=input_resolution_stage2, depth=depth[1], attention_ratio=attention_ratio[1], mlp_ratio=mlp_ratio[1], drop_path=drop_path, name='stage2')(x)\n",
    "\n",
    "x = levit_downsample(input_dim=embed_dim[1], output_dim=embed_dim[2], resolution=input_resolution_stage2, resolution_out=input_resolution_stage3, key_dim=down_ops[1][0], num_heads=down_ops[1][1], attn_ratio=down_ops[1][2], mlp_ratio=down_ops[1][3], stride=down_ops[1][4], drop_path=drop_path)(x)\n",
    "\n",
    "x = levit_stage(embed_dim=embed_dim[2], key_dim=key_dim[2], num_attention_heads=num_heads[2], resolution=input_resolution_stage3, depth=depth[2], attention_ratio=attention_ratio[2], mlp_ratio=mlp_ratio[2], drop_path=drop_path, name='stage3')(x)\n",
    "\n",
    "x = tf.math.reduce_mean(x, -1)\n",
    "x = NormLinear(num_classes)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\223044729\\Documents\\GitHub\\fastestimator\\apphub\\image_classification\\levit\\levit_test.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/223044729/Documents/GitHub/fastestimator/apphub/image_classification/levit/levit_test.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tf\u001b[39m.\u001b[39;49mones((\u001b[39m10\u001b[39;49m, \u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m, \u001b[39m10\u001b[39;49m))\u001b[39m.\u001b[39;49msoftmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:443\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mravel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtranspose\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    435\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtolist\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    436\u001b[0m   \u001b[39m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[0;32m    437\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    438\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m    439\u001b[0m \u001b[39m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[0;32m    440\u001b[0m \u001b[39m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[0;32m    441\u001b[0m \u001b[39m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[0;32m    442\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"\u001b[39m)\n\u001b[1;32m--> 443\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "tf.ones((10, 224, 224, 10)).softmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "input_data = torch.ones(4, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embded_output = model.patch_embed(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 14, 14])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embded_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embded_output1 = patch_embded_output.flatten(2).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196, 256])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embded_output1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNorm(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, bn_weight_init=1):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features, out_features, bias=False)\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(out_features)\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, bn_weight_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return self.batch_norm(x.flatten(0, 1)).reshape_as(x)\n",
    "\n",
    "queries_keys_values = LinearNorm(256, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196, 512])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_keys_values(patch_embded_output1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.reshape(tf.ones((4, 196, 512)), (4, 196, 4, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_output = queries_keys_values(patch_embded_output1).view(4, 196, 4, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196, 4, 128])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query, key, value = reshape_output.split([32, 32, 2 * 32], dim=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "query, key, value = tf.split(data, [32, 32, 2 * 32], axis=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 196, 4, 32]),\n",
       " torch.Size([4, 196, 4, 32]),\n",
       " torch.Size([4, 196, 4, 64]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([4, 196, 4, 32]),\n",
       " TensorShape([4, 196, 4, 32]),\n",
       " TensorShape([4, 196, 4, 64]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query.permute(0, 2, 1, 3)\n",
    "key = key.permute(0, 2, 1, 3)\n",
    "value = value.permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4, 196, 32]),\n",
       " torch.Size([4, 4, 196, 32]),\n",
       " torch.Size([4, 4, 196, 64]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 32, 196])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key.transpose(-2, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = query @ key.transpose(-2, -1) * 32**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 196, 196])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1767766952966369"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "resolution = 14\n",
    "points = list(itertools.product(range(resolution), range(resolution)))\n",
    "\n",
    "len_points = len(points)\n",
    "attention_offsets, indices = {}, []\n",
    "for p1 in points:\n",
    "    for p2 in points:\n",
    "        offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n",
    "        if offset not in attention_offsets:\n",
    "            attention_offsets[offset] = len(attention_offsets)\n",
    "        indices.append(attention_offsets[offset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38416"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "@tf.function\n",
    "def hard_swish(features):\n",
    "    \"\"\"Computes a hard version of the swish function.\n",
    "\n",
    "    This operation can be used to reduce computational cost and improve\n",
    "    quantization for edge devices.\n",
    "\n",
    "    Args:\n",
    "        features: A `Tensor` representing preactivation values.\n",
    "\n",
    "    Returns:\n",
    "        The activation value.\n",
    "    \"\"\"\n",
    "    return features * tf.nn.relu6(features + tf.cast(3., features.dtype)) * (1. / 6.)\n",
    "\n",
    "class Backbone(layers.Layer):\n",
    "    def __init__(self, out_channels):\n",
    "        super(Backbone, self).__init__(name='backbone')\n",
    "        self.convolution_layer1 = layers.Conv2D(filters=out_channels // 8,\n",
    "                                                kernel_size=3,\n",
    "                                                strides=2,\n",
    "                                                padding=\"same\",\n",
    "                                                use_bias=False,\n",
    "                                                name='convolution_layer1')\n",
    "        self.batch_norm1 = layers.BatchNormalization(gamma_initializer='ones', name='batch_norm1')\n",
    "        self.convolution_layer2 = layers.Conv2D(filters=out_channels // 4,\n",
    "                                                kernel_size=3,\n",
    "                                                strides=2,\n",
    "                                                padding=\"same\",\n",
    "                                                use_bias=False,\n",
    "                                                name='convolution_layer2')\n",
    "        self.batch_norm2 = layers.BatchNormalization(gamma_initializer='ones', name='batch_norm2')\n",
    "        self.convolution_layer3 = layers.Conv2D(filters=out_channels // 2,\n",
    "                                                kernel_size=3,\n",
    "                                                strides=2,\n",
    "                                                padding=\"same\",\n",
    "                                                use_bias=False,\n",
    "                                                name='convolution_layer3')\n",
    "        self.batch_norm3 = layers.BatchNormalization(gamma_initializer='ones', name='batch_norm3')\n",
    "        self.convolution_layer4 = layers.Conv2D(filters=out_channels,\n",
    "                                                kernel_size=3,\n",
    "                                                strides=2,\n",
    "                                                padding=\"same\",\n",
    "                                                use_bias=False,\n",
    "                                                name='convolution_layer4')\n",
    "        self.batch_norm4 = layers.BatchNormalization(gamma_initializer='ones', name='batch_norm4')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = hard_swish(self.batch_norm1(self.convolution_layer1(x)))\n",
    "        x = hard_swish(self.batch_norm2(self.convolution_layer2(x)))\n",
    "        x = hard_swish(self.batch_norm3(self.convolution_layer3(x)))\n",
    "        x = hard_swish(self.batch_norm4(self.convolution_layer4(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Backbone(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone/convolution_layer1/kernel:0\n",
      "backbone/batch_norm1/gamma:0\n",
      "backbone/batch_norm1/beta:0\n",
      "backbone/convolution_layer2/kernel:0\n",
      "backbone/batch_norm2/gamma:0\n",
      "backbone/batch_norm2/beta:0\n",
      "backbone/convolution_layer3/kernel:0\n",
      "backbone/batch_norm3/gamma:0\n",
      "backbone/batch_norm3/beta:0\n",
      "backbone/convolution_layer4/kernel:0\n",
      "backbone/batch_norm4/gamma:0\n",
      "backbone/batch_norm4/beta:0\n",
      "backbone/batch_norm1/moving_mean:0\n",
      "backbone/batch_norm1/moving_variance:0\n",
      "backbone/batch_norm2/moving_mean:0\n",
      "backbone/batch_norm2/moving_variance:0\n",
      "backbone/batch_norm3/moving_mean:0\n",
      "backbone/batch_norm3/moving_variance:0\n",
      "backbone/batch_norm4/moving_mean:0\n",
      "backbone/batch_norm4/moving_variance:0\n"
     ]
    }
   ],
   "source": [
    "for weight in model.weights:\n",
    "    print(weight.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 14, 14, 256])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tf.ones((3, 224, 224, 3))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ..., 193, 194, 195],\n",
       "        [  1,   0,   1,  ..., 192, 193, 194],\n",
       "        [  2,   1,   0,  ..., 191, 192, 193],\n",
       "        ...,\n",
       "        [193, 192, 191,  ...,   0,   1,   2],\n",
       "        [194, 193, 192,  ...,   1,   0,   1],\n",
       "        [195, 194, 193,  ...,   2,   1,   0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor(indices).view(len_points, len_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "data = tf.reshape(indices, (len_points, len_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.backing_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_biases = torch.nn.Parameter(torch.zeros(4, len(attention_offsets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_bias_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_bias_cache['cpu'] = attention_biases[:, torch.LongTensor(indices).view(len_points, len_points)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ..., 193, 194, 195],\n",
       "        [  1,   0,   1,  ..., 192, 193, 194],\n",
       "        [  2,   1,   0,  ..., 191, 192, 193],\n",
       "        ...,\n",
       "        [193, 192, 191,  ...,   0,   1,   2],\n",
       "        [194, 193, 192,  ...,   1,   0,   1],\n",
       "        [195, 194, 193,  ...,   2,   1,   0]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor(indices).view(len_points, len_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_bias_cache['cpu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73b4b63fab75275cc22557d8583617f6e244a28eab482e785f2d595a4709032f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
