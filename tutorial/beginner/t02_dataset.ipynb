{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Creating a FastEstimator dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Dataset in FastEstimator is a class that wraps raw input data and makes it easier to ingest by your code. In this tutorial we will learn about the different ways we can create these Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating simple datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FastEstimator Dataset class inherited from the PyTorch Dataset class which provides a clean and efficient interface to load raw data. Thus, all your code that worked for PyTorch will continue to work for FastEstimator too. For a refresher on the PyTorch Dataset you can go [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "In this tutorial we will focus on two key functionalities that we need to provide for the Dataset class. The first one is the ability to get an individual data entry from the Dataset and the second one is the ability to get the length of the Dataset. This is done as follows:\n",
    "* len(dataset) should return the size of the dataset.\n",
    "* dataset[i] should return the ith sample in the dataset.\n",
    "\n",
    "Lets create a simple PyTorch Dataset which shows this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class mydataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return self.data['x'].shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: self.data[key][idx] for key in self.data}\n",
    "a = {'x': np.random.rand(100,5), 'y': np.random.rand(100)}\n",
    "ds = mydataset(a)\n",
    "ds[0]\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastEstimator Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will showcase how a Dataset can be created using FastEstimator. This tutorial shows three ways you can create a Dataset in FastEstimator, first is using data from disk, the second one showing the creation of Dataset from memory and the last one uses a generator to create the Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating Dataset from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a Dataset from disk, you can either use a Labeled directory structure or read a CSV file on the disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Creating Dataset from Labeled directory structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To showcase this we will first have to create a dummy directory structure representing the two classes. Then we create a few files in each of the directories. The following image shows how the temp directory structure looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../image/t02_dataset_folder_structure.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet shows how we can replicate the directory structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import fastestimator as fe\n",
    "\n",
    "tmpdirname = tempfile.mkdtemp()\n",
    "\n",
    "a_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname)\n",
    "b_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname)\n",
    "\n",
    "a1 = open(os.path.join(a_tmpdirname.name, \"a1.txt\"), \"x\")\n",
    "a2 = open(os.path.join(a_tmpdirname.name, \"a2.txt\"), \"x\")\n",
    "\n",
    "b1 = open(os.path.join(b_tmpdirname.name, \"b1.txt\"), \"x\")\n",
    "b2 = open(os.path.join(b_tmpdirname.name, \"b2.txt\"), \"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is done, all you have to do is create a Dataset by passing the dummy directory to the LabeledDirDataset class constructor. The following code snippet shows how this can be done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': '/tmp/tmpd33g5011/tmpei7m6wy4/b1.txt', 'y': 0}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "dataset = fe.dataset.LabeledDirDataset(root_dir=tmpdirname)\n",
    "\n",
    "print (dataset[0])\n",
    "print (len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Creating Dataset from CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To showcase creating Dataset from CSV we now create a dummy CSV file representing information for the two classes. First, lets create the data to be used as input as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "\n",
    "import fastestimator as fe\n",
    "\n",
    "tmpdirname = tempfile.mkdtemp()\n",
    "\n",
    "data = {'x': ['a1.txt', 'a2.txt', 'b1.txt', 'b2.txt'], 'y': [0, 0, 1, 1]}\n",
    "df = pd.DataFrame(data=data)\n",
    "df.to_csv(os.path.join(tmpdirname, 'data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is done you can create a Dataset by passing the CSV to the CSVDataset class constructor. The following code snippet shows how this can be done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 'a1.txt', 'y': 0}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "dataset = fe.dataset.CSVDataset(file_path=os.path.join(tmpdirname, 'data.csv'))\n",
    "\n",
    "print (dataset[0])\n",
    "print (len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating Dataset from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to create a Dataset from data stored in memory. The following two sections demonstrate how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Creating a NumpyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a Dataset from memory, you use the NumpyDataset class passing it the data dictionary. The following code snippet shows how this can be done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import fastestimator as fe\n",
    "\n",
    "(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()\n",
    "train_data = fe.dataset.NumpyDataset({\"x\": x_train, \"y\": y_train})\n",
    "eval_data = fe.dataset.NumpyDataset({\"x\": x_eval, \"y\": y_eval})\n",
    "\n",
    "print (train_data[0]['y'])\n",
    "print (len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating a Dataset from Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to create a Dataset using generators. To do this, we will first create a generator which will generate random input data for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def inputs():\n",
    "    while True:\n",
    "        yield {'x': np.random.rand(4), 'y':np.random.randint(2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass the generator as an argument to the GeneratorDataset class. The following code snippet showcases how this can be done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([0.94845659, 0.62759004, 0.21577509, 0.88809381]), 'y': 1}\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from fastestimator.dataset import GeneratorDataset\n",
    "\n",
    "dataset = GeneratorDataset(generator=inputs(), samples_per_epoch=10)\n",
    "print (dataset[0])\n",
    "print (len(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python36964bitvenvvenv2f4b8d032e41422d9204c9b0dc473283"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
