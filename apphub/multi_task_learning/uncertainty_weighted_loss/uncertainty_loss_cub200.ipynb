{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task learning: using uncertainty weighted loss \n",
    "\n",
    "Multi-task learning is popular in many deep learning applications, for example, in object detection, the network performs both classification and localization for each object. As a result, the final loss will be a combination of classification loss and regression loss. The most frequent way of combining two losses is by simply adding them together. \n",
    "\n",
    "$ loss_{total} = loss_1 + loss_2 $\n",
    "\n",
    "\n",
    "However, problem emerges when the two losses are on different numerical scale. To resolve this issue, people usually manually design/experiemnt the best weight, which is very time consuming and computationally expensive.\n",
    "\n",
    "$ loss_{total} = w_1loss_1 + w_2loss_2 $\n",
    "\n",
    "[This paper](https://arxiv.org/abs/1705.07115) presents an interesting idea of making the weight w1 and w2 as trainable parameters based on uncertainty of each task, such that the network can dynamically focus more on the task with higher uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import fastestimator as fe\n",
    "from fastestimator import RecordWriter\n",
    "from fastestimator.architecture.uncertaintyloss import UncertaintyLoss\n",
    "from fastestimator.architecture.unet import UNet\n",
    "from fastestimator.dataset import cub200\n",
    "from fastestimator.op import NumpyOp\n",
    "from fastestimator.op.numpyop import ImageReader, MatReader, Reshape, Resize\n",
    "from fastestimator.op.tensorop import Augmentation2D, BinaryCrossentropy, Loss, Minmax, ModelOp, Rescale, \\\n",
    "SparseCategoricalCrossentropy\n",
    "from fastestimator.schedule.lr_scheduler import CyclicLRSchedule\n",
    "from fastestimator.trace import Accuracy, Dice, LRController, ModelSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "epochs = 25\n",
    "batch_size = 8\n",
    "steps_per_epoch = None\n",
    "validation_steps = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will use the [CUB200 2010 dataset](http://www.vision.caltech.edu/visipedia/CUB-200.html) by Caltech, it contains 6033 bird images from 200 categories, each image has its corresponding mask with it. Therefore, our task is to classify and segment the bird given the image.\n",
    "\n",
    "The cub200 dataset API will generate a summary CSV file for the data. The path of the csv file is returned as `csv_path`. The dataset path is returned as `path`. Inside the CSV file, the file paths are all relative to `path`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "csv_path, path = cub200.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Data pipeline\n",
    "\n",
    "The `RecordWriter` will convert the data into TFRecord. You can specify your data preprocessing with the `Preprocess` ops before saving into TFRecord. \n",
    "\n",
    "Here the main task is to resize the images and masks into 512 by 512 pixels. The image and mask preprocessings are \n",
    "\n",
    "- **image**  \n",
    "ImageReader &rarr; Resize\n",
    "\n",
    "- **mask**    \n",
    "MatReader &rarr; SelectDictKey (select only the mask info from MAT) &rarr; Resize &rarr; Reshape (add channel dimension for convenience of loss calculation)\n",
    "\n",
    "\n",
    "We read the JPG images with `ImageReader`, the masks stored in MAT file with `MatReader`. There is other information stored in the MAT file, so we use the custom `SelectDictKey` op to retrieve the mask only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/019.Gray_Catbird/Gray_Catbird_0010_29499503.jpg\n",
      "images/151.Black_capped_Vireo/Black_capped_Vireo_0014_2514410268.jpg\n",
      "images/133.White_throated_Sparrow/White_throated_Sparrow_0030_153904817.jpg\n",
      "images/007.Parakeet_Auklet/Parakeet_Auklet_0024_846242681.jpg\n"
     ]
    }
   ],
   "source": [
    "class SelectDictKey(NumpyOp):\n",
    "    def forward(self, data, state):\n",
    "        data = data['seg']\n",
    "        return data\n",
    "\n",
    "class DebugOp(NumpyOp):\n",
    "    def forward(self, data, state):\n",
    "        print(data)\n",
    "        return data\n",
    "\n",
    "writer = RecordWriter(\n",
    "    save_dir=os.path.join(path, \"tfrecords\"),\n",
    "    train_data=csv_path,\n",
    "    validation_data=0.2,\n",
    "    ops=[\n",
    "        DebugOp(inputs=\"image\", outputs=\"image\"),\n",
    "        ImageReader(inputs='image', parent_path=path),\n",
    "        Resize(target_size=(512, 512), keep_ratio=True, outputs='image'),\n",
    "        MatReader(inputs='annotation', parent_path=path),\n",
    "        SelectDictKey(),\n",
    "        Resize((512, 512), keep_ratio=True),\n",
    "        Reshape(shape=(512, 512, 1), outputs=\"annotation\")\n",
    "    ])\n",
    "\n",
    "\n",
    "# writer.write()\n",
    "\n",
    "# sample_data = {\"image\": [\"images/007.Parakeet_Auklet/Parakeet_Auklet_0024_846242681.jpg\", \"images/097.Orchard_Oriole/Orchard_Oriole_0024_2599230832.jpg\", \"images/151.Black_capped_Vireo/Black_capped_Vireo_0014_2514410268.jpg\", \"images/028.Brown_Creeper/Brown_Creeper_0011_114863052.jpg\", \"images/197.Marsh_Wren/Marsh_Wren_0019_2932534269.jpg\", \"images/082.Ringed_Kingfisher/Ringed_Kingfisher_0019_2289718893.jpg\"],\n",
    "#                 \"label\": [191, 165, 168, 187, 60, 13],\n",
    "#                \"annotation\": [\"annotations-mat/007.Parakeet_Auklet/Parakeet_Auklet_0024_846242681.mat\", \"annotations-mat/097.Orchard_Oriole/Orchard_Oriole_0024_2599230832.mat\", \"annotations-mat/151.Black_capped_Vireo/Black_capped_Vireo_0014_2514410268.mat\", \"annotations-mat/028.Brown_Creeper/Brown_Creeper_0011_114863052.mat\", \"annotations-mat/197.Marsh_Wren/Marsh_Wren_0019_2932534269.mat\", \"annotations-mat/082.Ringed_Kingfisher/Ringed_Kingfisher_0019_2289718893.mat\"]}\n",
    "\n",
    "sample_data = {\"image\": [\"images/019.Gray_Catbird/Gray_Catbird_0010_29499503.jpg\", \"images/151.Black_capped_Vireo/Black_capped_Vireo_0014_2514410268.jpg\", \"images/133.White_throated_Sparrow/White_throated_Sparrow_0030_153904817.jpg\", \"images/007.Parakeet_Auklet/Parakeet_Auklet_0024_846242681.jpg\"],\n",
    "                \"label\": [143, 168, 94, 191],\n",
    "               \"annotation\": [\"annotations-mat/019.Gray_Catbird/Gray_Catbird_0010_29499503.mat\", \"annotations-mat/151.Black_capped_Vireo/Black_capped_Vireo_0014_2514410268.mat\", \"annotations-mat/133.White_throated_Sparrow/White_throated_Sparrow_0030_153904817.mat\", \"annotations-mat/007.Parakeet_Auklet/Parakeet_Auklet_0024_846242681.mat\"]}\n",
    "\n",
    "\n",
    "result = writer.transform(data=sample_data, mode=\"train\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result[\"image\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can send this `RecordWriter` instance to `Pipeline`. `Pipeline` reads the TFRecord generated from `RecordWriter` and applies further transformation specified in the `ops` argument on-the-fly during training. \n",
    "\n",
    "We will augmente both image and mask in the same way, apply random rotation from -15 to 15 degree , then randomly zoom between 0.8 to 1.2 and then randomly rotate the image. Finally, rescale the image piexel value between -1 to 1 since we are using pretrained ImageNet weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = fe.Pipeline(\n",
    "    batch_size=batch_size,\n",
    "    data=writer,\n",
    "    ops=[\n",
    "        Augmentation2D(inputs=(\"image\", \"annotation\"),\n",
    "                       outputs=(\"image\", \"annotation\"),\n",
    "                       mode=\"train\",\n",
    "                       rotation_range=15.0,\n",
    "                       zoom_range=[0.8, 1.2],\n",
    "                       flip_left_right=True),\n",
    "        Rescale(inputs='image', outputs='image')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize our pipeline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def Minmax(data):\n",
    "    data_max = np.max(data)\n",
    "    data_min = np.min(data)\n",
    "    data = (data - data_min) / max((data_max - data_min), 1e-7)\n",
    "    return data\n",
    "\n",
    "\n",
    "def visualize_image_mask(img, mask):\n",
    "    img = (img*255).astype(np.uint8)\n",
    "\n",
    "    mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n",
    "    mask = (mask).astype(np.uint8)\n",
    "\n",
    "    ret, mask_thres = cv2.threshold(mask, 0.5,1, cv2.THRESH_BINARY)\n",
    "    mask_overlay = mask * mask_thres\n",
    "    mask_overlay = np.where( mask_overlay != [0,0,0], [255,0,0] ,[0,0,0])\n",
    "    mask_overlay = mask_overlay.astype(np.uint8)\n",
    "    img_with_mask = cv2.addWeighted(img, 0.7, mask_overlay, 0.3,0 )\n",
    "\n",
    "    maskgt_with_maskpred = cv2.addWeighted(mask, 0.7, mask_overlay, 0.3, 0)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(18,8))\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title('original lung')\n",
    "    ax[1].imshow(img_with_mask)\n",
    "    ax[1].set_title('img - predict mask ')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator: Saving tfrecord to /Users/212711536/fastestimator_data/CUB200/tfrecords\n",
      "FastEstimator: Converting Train TFRecords 0.0%, Speed: 0.00 record/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/concurrent/futures/process.py\", line 295, in _queue_management_worker\n",
      "    shutdown_worker()\n",
      "  File \"/anaconda3/lib/python3.6/concurrent/futures/process.py\", line 253, in shutdown_worker\n",
      "    call_queue.put_nowait(None)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 129, in put_nowait\n",
      "    return self.put(obj, False)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 83, in put\n",
      "    raise Full\n",
      "queue.Full\n",
      "\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8c49fee77504>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimg_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmask_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"annotation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Opensource/fastestimator-1.0/fastestimator/pipeline.py\u001b[0m in \u001b[0;36mshow_results\u001b[0;34m(self, mode, num_steps, current_epoch, reuse)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_batch_multiplier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_num_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_prepared\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0mds_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_schedule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Opensource/fastestimator-1.0/fastestimator/pipeline.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Opensource/fastestimator-1.0/fastestimator/record_writer.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, save_dir)\u001b[0m\n\u001b[1;32m    144\u001b[0m         for train_data, validation_data, write_feature, ops in zip(self.train_data, self.validation_data,\n\u001b[1;32m    145\u001b[0m                                                                    self.write_feature, self.ops):\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_record_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_set_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Opensource/fastestimator-1.0/fastestimator/record_writer.py\u001b[0m in \u001b[0;36m_create_record_local\u001b[0;34m(self, train_data, validation_data, write_feature, ops)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_feature_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_data_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_example_record\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_tfrecord_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_json_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Opensource/fastestimator-1.0/fastestimator/record_writer.py\u001b[0m in \u001b[0;36m_write_tfrecord_parallel\u001b[0;34m(self, dictionary, mode)\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mfile_idx_start\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_files_process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0mnum_example_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mfeature_shape_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(low=0, high=batch_size)\n",
    "result = pipeline.show_results()\n",
    "img_batch = result[0][\"image\"].numpy()\n",
    "mask_batch = result[0][\"annotation\"].numpy()\n",
    "img = Minmax(img_batch[idx])\n",
    "msk = np.squeeze(mask[idx])\n",
    "visualize_image_mask(img, msk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "In this implementation, the network architecture is not the focus, therefore, we are going to create something out of the blue :), how about a combination of resnet50 and Unet that can do both classification and segmentation? we can call it - ResUnet50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResUnet50(input_shape=(512, 512, 3), num_classes=200):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    resnet50 = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, pooling=None, input_tensor=inputs)\n",
    "    assert resnet50.layers[4].name == \"conv1_relu\"\n",
    "    C1 = resnet50.layers[4].output  # 256 x 256 x 64\n",
    "    assert resnet50.layers[38].name == \"conv2_block3_out\"\n",
    "    C2 = resnet50.layers[38].output  # 128 x 128 x 256\n",
    "    assert resnet50.layers[80].name == \"conv3_block4_out\"\n",
    "    C3 = resnet50.layers[80].output  # 64 x 64 x 512\n",
    "    assert resnet50.layers[142].name == \"conv4_block6_out\"\n",
    "    C4 = resnet50.layers[142].output  # 32 x 32 x 1024\n",
    "    assert resnet50.layers[-1].name == \"conv5_block3_out\"\n",
    "    C5 = resnet50.layers[-1].output  # 16 x 16 x 2048\n",
    "    #classification subnet\n",
    "    label = layers.GlobalMaxPool2D()(C5)\n",
    "    label = layers.Flatten()(label)\n",
    "    label = layers.Dense(num_classes, activation='softmax')(label)\n",
    "    #segmentation subnet\n",
    "    conv_config = {'activation': 'relu', 'padding': 'same', 'kernel_initializer': 'he_normal'}\n",
    "    up6 = layers.Conv2D(512, 3, **conv_config)(layers.UpSampling2D(size=(2, 2))(C5))  # 32 x 32 x 512\n",
    "    merge6 = layers.concatenate([C4, up6], axis=3)  # 32 x 32 x 1536\n",
    "    conv6 = layers.Conv2D(512, 3, **conv_config)(merge6)  # 32 x 32 x 512\n",
    "    conv6 = layers.Conv2D(512, 3, **conv_config)(conv6)  # 32 x 32 x 512\n",
    "    up7 = layers.Conv2D(256, 3, **conv_config)(layers.UpSampling2D(size=(2, 2))(conv6))  # 64 x 64 x 256\n",
    "    merge7 = layers.concatenate([C3, up7], axis=3)  # 64 x 64 x 768\n",
    "    conv7 = layers.Conv2D(256, 3, **conv_config)(merge7)  # 64 x 64 x 256\n",
    "    conv7 = layers.Conv2D(256, 3, **conv_config)(conv7)  # 64 x 64 x 256\n",
    "    up8 = layers.Conv2D(128, 3, **conv_config)(layers.UpSampling2D(size=(2, 2))(conv7))  # 128 x 128 x 128\n",
    "    merge8 = layers.concatenate([C2, up8], axis=3)  # 128 x 128 x 384\n",
    "    conv8 = layers.Conv2D(128, 3, **conv_config)(merge8)  # 128 x 128 x 128\n",
    "    conv8 = layers.Conv2D(128, 3, **conv_config)(conv8)  # 128 x 128 x 128\n",
    "    up9 = layers.Conv2D(64, 3, **conv_config)(layers.UpSampling2D(size=(2, 2))(conv8))  # 256 x 256 x 64\n",
    "    merge9 = layers.concatenate([C1, up9], axis=3)  # 256 x 256 x 128\n",
    "    conv9 = layers.Conv2D(64, 3, **conv_config)(merge9)  # 256 x 256 x 64\n",
    "    conv9 = layers.Conv2D(64, 3, **conv_config)(conv9)  # 256 x 256 x 64\n",
    "    up10 = layers.Conv2D(2, 3, **conv_config)(layers.UpSampling2D(size=(2, 2))(conv9))  # 512 x 512 x 2\n",
    "    mask = layers.Conv2D(1, 1, activation='sigmoid')(up10)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[label, mask])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
