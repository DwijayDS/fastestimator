{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tutorial 2: Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginner's tutorial of `Pipeline`, we learned how to build data pipeline that handles data loading and preprocessing tasks efficiently. Now that you have understood some basic operations in the `Pipeline`, we will demonstrate some advanced concepts and how to leverage them to create efficient `Pipeline` in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will discuss following topics,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How to iterate through the pipeline data\n",
    "    * Basic concept\n",
    "    * Example use case\n",
    "* Dropping the last batch\n",
    "* Handling the batch padding\n",
    "* How to benchmark Pipeline performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to iterate through the pipeline data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first see how to iterate through the pipeline batch data. For example if we want to calculate global mean of pixel value or standard deviation over the channels we could iterate through the batch data and compute them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create sample NumpyDataset from the data dictionary and load it into `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fastestimator.dataset.data import cifar10\n",
    "    \n",
    "# sample numpy array to later create datasets from them\n",
    "x_train, y_train = (np.random.sample((10, 2)), np.random.sample((10, 1)))\n",
    "train_data = {\"x\": x_train, \"y\": y_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.dataset.numpy_dataset import NumpyDataset\n",
    "\n",
    "# create NumpyDataset from the sample data\n",
    "dataset_fe = NumpyDataset(train_data)\n",
    "\n",
    "pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the loader object for the `Pipeline` we defined and iterate over the dataset that was loaded into the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[0.4575, 0.8058],\n",
      "        [0.8462, 0.1682],\n",
      "        [0.1016, 0.3228]], dtype=torch.float64), 'y': tensor([[0.8675],\n",
      "        [0.0056],\n",
      "        [0.3656]], dtype=torch.float64)}\n",
      "{'x': tensor([[0.6502, 0.7932],\n",
      "        [0.5179, 0.5414],\n",
      "        [0.9607, 0.0284]], dtype=torch.float64), 'y': tensor([[0.6766],\n",
      "        [0.4403],\n",
      "        [0.3337]], dtype=torch.float64)}\n",
      "{'x': tensor([[0.5675, 0.8176],\n",
      "        [0.9654, 0.8325],\n",
      "        [0.0961, 0.1680]], dtype=torch.float64), 'y': tensor([[0.8057],\n",
      "        [0.9169],\n",
      "        [0.2998]], dtype=torch.float64)}\n",
      "{'x': tensor([[0.8078, 0.3384]], dtype=torch.float64), 'y': tensor([[0.2901]], dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "loader_fe = pipeline_fe.get_loader(mode=\"train\")\n",
    "\n",
    "for batch in loader_fe:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have CIFAR-10 dataset and we want to find global average pixel value over three channels then we can loop through the batch data and quickly compute the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data import cifar10\n",
    "\n",
    "cifar_train, _ = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take the `batch_size` 64 and load the data into `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cifar = fe.Pipeline(train_data=cifar_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will iterate through batch data and compute the mean pixel values for all three channels of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_fe = pipeline_cifar.get_loader(mode=\"train\", shuffle=False)\n",
    "mean_arr = np.zeros((3))\n",
    "for i, batch in enumerate(loader_fe):\n",
    "    mean_arr = mean_arr + np.mean(batch[\"x\"].numpy(), axis=(0, 1, 2))\n",
    "mean_arr = mean_arr / (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean pixel value over the channels:  [125.32287898 122.96682199 113.8856495 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean pixel value over the channels: \", mean_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping the last batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we specify `batch_size` in the `Pipeline`, it will combine consecutive number of tensors into a batch and resulting shape will be <br><b>batch_size * shape of input tensor</b><br> However, if `batch_size` does not divide the input data evenly then last batch could have different batch_size than other batches.<br>\n",
    "To drop the last batch we can set `drop_last` to `True`. Therefore, if the last batch is incomplete it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling the batch padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we saw that if last batch has different shape than rest of the batches then we can drop the last batch. But there might be scenario where the input tensors that are batched have different dimensions i.e. In Natural language processing problems we can have input strings can have different lengths. For that the tensors are padded out to the maximum length of the all the tensors in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will take numpy array that contains different shapes of array elements and load it into the `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define numpy arrays with different shapes\n",
    "elem1 = np.array([4, 5])\n",
    "elem2 = np.array([1, 2, 6])\n",
    "elem3 = np.array([3])\n",
    "\n",
    "# create train dataset\n",
    "x_train = np.array([elem1, elem2, elem3])\n",
    "train_data = {\"x\": x_train}\n",
    "dataset_fe = NumpyDataset(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set any `pad_value` that we want to append at the end of the tensor data. `pad_value` must be either `int` or `float`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3, pad_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's iterate over the batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[4, 5, 0],\n",
      "        [1, 2, 6],\n",
      "        [3, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for elem in iter(pipeline_fe.get_loader(mode='train', shuffle=False)):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking pipeline performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ideal world, deep learning scientists would need to evaluate costs and speed in either in terms of data processing or model training before deploying. That makes benchmarking such tasks significant as we need good summary of the measures.<br>\n",
    "`Pipeline.benchmark` provides that important feature of benchmarking processing speed of pre-processing operations in the `Pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create `Pipeline` for the CIFAR-10 dataset with list of numpy operators that expand dimensions, apply minmax scaler and finally rotate the input images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.op.numpyop.univariate import Minmax, ExpandDims\n",
    "from fastestimator.op.numpyop.multivariate import Rotate\n",
    "\n",
    "pipeline = fe.Pipeline(train_data=cifar_train,\n",
    "                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"),\n",
    "                            Minmax(inputs=\"x\", outputs=\"x_out\"),\n",
    "                            Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=180)],\n",
    "                      batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark the processing speed in the training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastEstimator: Step: 100, Epoch: 1, Steps/sec: 306.3574085435541\n",
      "FastEstimator: Step: 200, Epoch: 1, Steps/sec: 440.5841906691682\n",
      "FastEstimator: Step: 300, Epoch: 1, Steps/sec: 458.66033407201814\n",
      "FastEstimator: Step: 400, Epoch: 1, Steps/sec: 423.8592310935567\n",
      "FastEstimator: Step: 500, Epoch: 1, Steps/sec: 457.58897449238594\n",
      "FastEstimator: Step: 600, Epoch: 1, Steps/sec: 439.4676858001863\n",
      "FastEstimator: Step: 700, Epoch: 1, Steps/sec: 412.746418382437\n"
     ]
    }
   ],
   "source": [
    "pipeline_cifar.benchmark(mode=\"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fe",
   "language": "python",
   "name": "fe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
