{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tutorial 1: Dataset\n",
    "\n",
    "## Overview\n",
    "In this tutorial, we will talk about the following topics:\n",
    "* Dataset summary\n",
    "* Dataset splitting\n",
    "    * Random fraction split\n",
    "    * Random count split\n",
    "    * Index split\n",
    "* Global Dataset Editing\n",
    "* BatchDataset\n",
    "    * Deterministic batching\n",
    "    * Distribution batching\n",
    "    * Unpaired dataset\n",
    "\n",
    "Before going through the tutorial, it is recommended to check [beginner tutorial 02](linkneeded) for basic understanding of `dataset` from Pytorch and FastEstimator. We will talk about more details about `fe.dataset` API in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset summary\n",
    "As we have mentioned in previous tutorial, users can import our inherited dataset class for easy use in `Pipeline`. But how do we know what keys are available in the dataset?   Well, obviously one easy way is just call `dataset[0]` and check the keys. However, there's a more elegant way to check information of dataset: `dataset.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data.mnist import load_data\n",
    "train_data, eval_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetSummary {'num_instances': 60000, 'keys': {'x': <KeySummary {'shape': [28, 28], 'dtype': 'uint8'}>, 'y': <KeySummary {'num_unique_values': 10, 'shape': [], 'dtype': 'uint8'}>}}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting\n",
    "\n",
    "dataset splitting is nothing new in machine learning, in FastEstimator, users can easily split their data in different ways:\n",
    "\n",
    "### random fraction split\n",
    "Let's say we want to randomly split 50% of evaluation data into test data, simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = eval_data.split(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if I want to split evaluation data into two test datasets with 20% of evaluation data each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata1, test_data2 = eval_data.split(0.2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random count split\n",
    "Sometimes instead of fractions, we want actual number of examples to split, for example, randomly splitting 100 samples from evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data3 = eval_data.split(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course, we can generate multiple datasets by providing multiple inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data4, test_data5 = eval_data.split(100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index split\n",
    "There are times when we need to split the dataset in a specific way, then you can provide the index: for example, if we want to split the 0th, 1st and 100th element of evaluation dataset into new test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data6 = eval_data.split([0,1,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you just want continuous index, here's an easy way to provide index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data7 = eval_data.split(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needless to say, you can provide multiple inputs too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data7, test_data8 = eval_data.split([0, 1 ,2], [3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Dataset Editting\n",
    "In deep learning, we usually process the dataset batch by batch. However, when we are handling the tabular data, we might need to apply some transformation globally before the training.  For example, standardize the tabular data using `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data.breast_cancer import load_data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_data, eval_data = load_data()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_data[\"x\"] = scaler.fit_transform(train_data[\"x\"])\n",
    "eval_data[\"x\"] = scaler.transform(eval_data[\"x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchDataset\n",
    "\n",
    "There might be scenarios where we need to combine multiple datasets together into one dataset in a specific way, next we will talk about 3 such use cases.\n",
    "\n",
    "### Deterministic batching\n",
    "Let's say we have `mnist` and `cifar` dataset, given the total batch size of 8, if we always want 4 examples from `mnist` and the rest from `cifar`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data import mnist, cifar10\n",
    "from fastestimator.dataset import BatchDataset\n",
    "\n",
    "mnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\")\n",
    "cifar_data, _ = cifar10.load_data(image_key=\"x\", label_key=\"y\")\n",
    "\n",
    "dataset_deterministic = BatchDataset(datasets=[mnist_data, cifar_data], num_samples=[4,4])\n",
    "# ready to use dataset_deterministic in Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution batching\n",
    "Some people who prefer randomness in a batch, for example, given total batch size of 8, we want 0.5 probability of `mnist` and the other 0.5 from `cifar`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data import mnist, cifar10\n",
    "from fastestimator.dataset import BatchDataset\n",
    "\n",
    "mnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\")\n",
    "cifar_data, _ = cifar10.load_data(image_key=\"x\", label_key=\"y\")\n",
    "\n",
    "dataset_distribution = BatchDataset(datasets=[mnist_data, cifar_data], num_samples=8, probability=[0.5, 0.5])\n",
    "# ready to use dataset_distribution in Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpaird dataset\n",
    "Some deep learning tasks require random unpaired dataset. For example, in image-to-image translation (like Cycle-GAN), it needs to randomly sample one horse image and one zebra image for every batch. In FastEstimator, `BatchDataset` can also handle unpaired dataset, all you need to make sure is: **keys from two different datasets must be unique for unpaired dataset**.\n",
    "\n",
    "For example, let's sample one image from `mnist` and one image from `cifar` for every batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.dataset.data import mnist, cifar10\n",
    "from fastestimator.dataset import BatchDataset\n",
    "\n",
    "mnist_data, _ = mnist.load_data(image_key=\"x_mnist\", label_key=\"y_mnist\")\n",
    "cifar_data, _ = cifar10.load_data(image_key=\"x_cifar\", label_key=\"y_cifar\")\n",
    "\n",
    "dataset_unpaired = BatchDataset(datasets=[mnist_data, cifar_data], num_samples=[1,1])\n",
    "# ready to use dataset_unpaired in Pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
