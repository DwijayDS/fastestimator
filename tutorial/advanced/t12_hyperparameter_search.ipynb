{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Tutorial 12: Hyperparameter Search\n",
    "\n",
    "## Overview\n",
    "In this tutorial, we will discuss the following topics:\n",
    "* [FastEstimator Search API](#ta12searchapi)\n",
    "    * [Getting the search results](#ta12searchresults)\n",
    "    * [Saving and loading search results](#ta12saveload)\n",
    "    * [Interruption-resilient search](#ta12interruption)\n",
    "* [Example 1: Hyperparameter Tuning by Grid Search](#ta12example1)\n",
    "    * [Search Visualization](#ta12searchvisualization)\n",
    "* [Example 2: RUA Augmentation via Golden-Section Search](#ta12example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta12searchapi'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search API\n",
    "\n",
    "There are many things in life that requires searching for an optimal solution in a given space, regardless of whether deep learning is involved. For example:\n",
    "* what is the `x` that leads to the minimal value of `(x-3)**2`?\n",
    "* what is the best `learning rate` and `batch size` combo that can produce the lowest evaluation loss after 2 epochs of training?\n",
    "* what is the best augmentation magnitude that can lead to the highest evaluation accuracy?\n",
    "\n",
    "The `fe.search` API is designed to make the search easier, the API can be used independently for any search problem, as it only requires the following two components:\n",
    "1. objective function to measure the score of a solution.\n",
    "2. whether a maximum or minimum score is desired.\n",
    "\n",
    "We will start with a simple example using `Grid Search`. Say we want to find the `x` that produces the minimal value of `(x-3)**2`, where x is chosen from the list: `[0.5, 1.5, 2.9, 4, 5.3]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.search import GridSearch\n",
    "\n",
    "def objective_fn(search_idx, x):\n",
    "    return {\"objective\": (x-3)**2}\n",
    "\n",
    "grid_search = GridSearch(eval_fn=objective_fn, params={\"x\": [0.5, 1.5, 2.9, 4, 5.3]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the score function, one of the arguments must be `search_idx`. This is to help user differentiate multiple search runs. To run the search, simply call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta12searchresults'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the search results\n",
    "After the search is done, you can also call the `search.get_best_results` or `search.get_search_results` to see the best and overall search history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best search result:\")\n",
    "print(grid_search.get_best_results(best_mode=\"min\", optimize_field=\"objective\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"search history:\")\n",
    "print(grid_search.get_search_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta12saveload'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading search results\n",
    "\n",
    "Once the search is done, you can also save the search results into the disk and later load them back using `save` and `load` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "save_dir = tempfile.mkdtemp()\n",
    "\n",
    "# save the state to save_dir\n",
    "grid_search.save(save_dir) \n",
    "\n",
    "# instantiate a new object\n",
    "grid_search2 = GridSearch(eval_fn=objective_fn, params={\"x\": [0.5, 1.5, 2.9, 4, 5.3]}) \n",
    "\n",
    "# load the previously saved state\n",
    "grid_search2.load(save_dir)\n",
    "\n",
    "# display the best result of the loaded instance\n",
    "print(grid_search2.get_best_results(best_mode=\"min\", optimize_field=\"objective\")) \n",
    "\n",
    "# display the search summary of the loadeded instance\n",
    "print(grid_search2.get_search_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta12interruption'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interruption-resilient search\n",
    "When you run search on a hardware that can be interrupted (like an AWS spot instance), you can provide a `save_dir` argument when calling `fit`. As a result, the search will automatically back up its result after each evaluation. Furthermore, when calling `fit` using the same `save_dir` the second time, it will first load the search results and then pick up from where it left off. \n",
    "\n",
    "To demonstrate this, we will use golden-section search on the same optimization problem. To simulate interruption, we will first iterate 10 times, then create a new instance and iterate another 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.search import GoldenSection\n",
    "save_dir2 = tempfile.mkdtemp()\n",
    "\n",
    "gs_search =  GoldenSection(eval_fn=objective_fn, \n",
    "                           x_min=0, \n",
    "                           x_max=6, \n",
    "                           max_iter=10, \n",
    "                           integer=False, \n",
    "                           optimize_field=\"objective\", \n",
    "                           best_mode=\"min\")\n",
    "\n",
    "gs_search.fit(save_dir=save_dir2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After interruption, we can create the instance and call `fit` on the same directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_search2 =  GoldenSection(eval_fn=objective_fn, \n",
    "                           x_min=0, \n",
    "                           x_max=6, \n",
    "                           max_iter=20, \n",
    "                           integer=False, \n",
    "                           optimize_field=\"objective\", \n",
    "                           best_mode=\"min\")\n",
    "\n",
    "gs_search2.fit(save_dir=save_dir2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the search started from search index 13 and proceeded for another 10 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta12example1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Hyperparameter Tuning by Grid Search\n",
    "\n",
    "In this example, we will use `GridSearch` on a real deep learning task to illustrate its usage. Based on number of hyperparameters, the  grid search is performed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import fastestimator as fe\n",
    "from fastestimator.architecture.tensorflow import LeNet\n",
    "from fastestimator.dataset.data import mnist\n",
    "from fastestimator.op.numpyop.univariate import ExpandDims, Minmax, RUA\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "\n",
    "\n",
    "def get_hypara_tuning_estimator(batch_size, lr, choice):\n",
    "\n",
    "    pipeline_ops = []\n",
    "\n",
    "    if choice and isinstance(choice, str):\n",
    "        pipeline_ops = [RUA(inputs=\"x\", outputs=\"x\", mode=\"train\", choices=[choice])]\n",
    "\n",
    "    pipeline_ops = pipeline_ops + [ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")]\n",
    "    train_data, test_data = mnist.load_data()\n",
    "    pipeline = fe.Pipeline(train_data=train_data,\n",
    "                           test_data=test_data,\n",
    "                           batch_size=batch_size,\n",
    "                           ops=pipeline_ops,\n",
    "                           num_process=0)\n",
    "    model = fe.build(model_fn=LeNet, optimizer_fn=lambda: tf.optimizers.Adam(lr))\n",
    "    network = fe.Network(ops=[\n",
    "        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n",
    "        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n",
    "        UpdateOp(model=model, loss_name=\"ce\")\n",
    "    ])\n",
    "    estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=1, train_steps_per_epoch=500)\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a batch size grid `[32, 64]`, we are interested in the optimial parameter that leads to the lowest test loss after 200 steps of training on MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn_v1(search_idx, batch_size):\n",
    "    est = get_hypara_tuning_estimator(batch_size, lr=1e-3, choice=None)\n",
    "    est.fit(warmup=False)\n",
    "    hist = est.test(summary=\"myexp\")\n",
    "    loss = float(hist.history[\"test\"][\"ce\"][500])\n",
    "    return {\"test_loss\": loss}\n",
    "\n",
    "\n",
    "mnist_grid_search_single = GridSearch(eval_fn=eval_fn_v1, params={\"batch_size\": [32, 64]})\n",
    "\n",
    "mnist_grid_search_single.fit()\n",
    "\n",
    "mnist_grid_search_single.get_best_results(best_mode=\"min\", optimize_field=\"test_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a batch size grid `[32, 64]` and learning rate grid `[1e-2 and 1e-3]`, we are interested in the optimial parameter that leads to the lowest test loss after 200 steps of training on MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn_v2(search_idx, batch_size, lr):\n",
    "    est = get_hypara_tuning_estimator(batch_size, lr=lr, choice=None)\n",
    "    est.fit(warmup=False)\n",
    "    hist = est.test(summary=\"myexp\")\n",
    "    loss = float(hist.history[\"test\"][\"ce\"][500])\n",
    "    return {\"test_loss\": loss}\n",
    "\n",
    "mnist_grid_search_double = GridSearch(eval_fn=eval_fn_v2, params={\"batch_size\": [32, 64], \"lr\": [1e-2, 1e-3]})\n",
    "\n",
    "mnist_grid_search_double.fit()\n",
    "\n",
    "mnist_grid_search_double.get_best_results(best_mode=\"min\", optimize_field=\"test_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a batch size grid `[32, 64]`, learning rate grid `[1e-2 and 1e-3]` and built-in augmentation `[\"Rotate\", \"Brightness\"]`, we are interested in the optimial parameter that leads to the lowest test loss after 200 steps of training on MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn_v3(search_idx, batch_size, lr, choices):\n",
    "    est = get_hypara_tuning_estimator(batch_size, lr=lr, choice=choices)\n",
    "    est.fit(warmup=False)\n",
    "    hist = est.test(summary=\"myexp\")\n",
    "    loss = float(hist.history[\"test\"][\"ce\"][500])\n",
    "    return {\"test_loss\": loss}\n",
    "\n",
    "mnist_grid_search_multi = GridSearch(\n",
    "    eval_fn=eval_fn_v3, params={\n",
    "        \"batch_size\": [32, 64], \"lr\": [1e-2, 1e-3], \"choices\": [\"Rotate\", \"Brightness\"]\n",
    "    })\n",
    "\n",
    "mnist_grid_search_multi.fit()\n",
    "\n",
    "mnist_grid_search_multi.get_best_results(best_mode=\"min\", optimize_field=\"test_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta12searchvisualization'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of grid search with single hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.search.visualize import visualize_search\n",
    "\n",
    "visualize_search(search=mnist_grid_search_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of grid search with two hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.search.visualize import visualize_search\n",
    "\n",
    "visualize_search(search=mnist_grid_search_double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of grid search with more than 2 hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.search.visualize import visualize_search\n",
    "\n",
    "visualize_search(search=mnist_grid_search_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta12example2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: RUA Augmentation via Golden-Section Search\n",
    "\n",
    "In this example, we will use a built-in augmentation NumpyOp - RUA - and find the optimial level between 0 to 30 using `Golden-Section` search. The test result will be evaluated on the ciFAIR10 dataset after 500 steps of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import fastestimator as fe\n",
    "from fastestimator.architecture.tensorflow import LeNet\n",
    "from fastestimator.dataset.data import cifair10\n",
    "from fastestimator.op.numpyop.univariate import ExpandDims, Minmax, RUA\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "\n",
    "def get_estimator(level):\n",
    "    train_data, test_data = cifair10.load_data()\n",
    "    pipeline = fe.Pipeline(train_data=train_data,\n",
    "                           test_data=test_data,\n",
    "                           batch_size=64,\n",
    "                           ops=[RUA(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"), \n",
    "                                Minmax(inputs=\"x\", outputs=\"x\")],\n",
    "                           num_process=0)\n",
    "    model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\")\n",
    "    network = fe.Network(ops=[\n",
    "        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n",
    "        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n",
    "        UpdateOp(model=model, loss_name=\"ce\")\n",
    "    ])\n",
    "    estimator = fe.Estimator(pipeline=pipeline,\n",
    "                             network=network,\n",
    "                             epochs=1,\n",
    "                             train_steps_per_epoch=500)\n",
    "    return estimator\n",
    "\n",
    "def eval_fn(search_idx, level):\n",
    "    est = get_estimator(level)\n",
    "    est.fit(warmup=False)\n",
    "    hist = est.test(summary=\"myexp\")\n",
    "    loss = float(hist.history[\"test\"][\"ce\"][500])\n",
    "    return {\"test_loss\": loss}\n",
    "\n",
    "cifair10_gs_search = GoldenSection(eval_fn=eval_fn, x_min=0, x_max=30, max_iter=5, best_mode=\"min\", optimize_field=\"test_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cifair10_gs_search.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the optimial level we found is 4. We can then train the model again using `level=4` to get the final model. In a real use case you will want to perform parameter search on a held-out evaluation set, and test the best parameters on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
