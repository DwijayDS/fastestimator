{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Getting Started\n",
    "\n",
    "## Overview\n",
    "Welcome to FastEstimator! In this tutorial we are going to cover:\n",
    "* [The three main APIs of FastEstimator: `Pipeline`, `Network`, `Estimator`](#t01ThreeMain)\n",
    "* [An image classification example](#t01ImageClassification)\n",
    "    * [Pipeline](#t01Pipeline)\n",
    "    * [Network](#t01Network)\n",
    "    * [Estimator](#t01Estimator)\n",
    "    * [Training](#t01Training)\n",
    "    * [Inferencing](#t01Inferencing)\n",
    "* [Related Apphub Examples](#t01Apphub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01ThreeMain'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three main APIs\n",
    "All deep learning training workï¬‚ows involve the following three essential components, each mapping to a critical API in FastEstimator.\n",
    "\n",
    "* **Data pipeline**: extracts data from disk/RAM, performs transformations. ->  `fe.Pipeline`\n",
    "\n",
    "\n",
    "* **Network**: performs trainable and differentiable operations. ->  `fe.Network`\n",
    "\n",
    "\n",
    "* **Training loop**: combines the data pipeline and network in an iterative process. ->  `fe.Estimator`\n",
    "\n",
    "<BR>\n",
    "<BR>\n",
    "Any deep learning task can be constructed by following the 3 main steps:\n",
    "<BR>\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/fastestimator-util/fastestimator-misc/blob/master/resource/pictures/tutorial/t01_api.png?raw=true\" alt=\"drawing\" width=\"700\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01ImageClassification'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Pipeline'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Pipeline\n",
    "We use FastEstimator dataset API to load the MNIST dataset. Please check out [Tutorial 2](./t02_dataset.ipynb) for more details about the dataset API. In this case our data preprocessing involves: \n",
    "1. Expand image dimension from (28,28) to (28, 28, 1) for convenience during convolution operations.\n",
    "2. Rescale pixel values from [0, 255] to [0, 1].\n",
    "\n",
    "Please check out [Tutorial 3](./t03_operator.ipynb) for details about `Operator` and [Tutorial 4](./t04_pipeline.ipynb) for `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 20:23:55.959144: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-01 20:23:56.682169: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-11-01 20:23:56.682252: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-11-01 20:23:56.682258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import fastestimator as fe\n",
    "from fastestimator.dataset.data import mnist\n",
    "from fastestimator.op.numpyop.univariate import ExpandDims, Minmax\n",
    "\n",
    "train_data, eval_data = mnist.load_data()\n",
    "\n",
    "pipeline = fe.Pipeline(train_data=train_data,\n",
    "                       eval_data=eval_data,\n",
    "                       batch_size=32,\n",
    "                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Network'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Network\n",
    "\n",
    "The model definition can be either from `tf.keras.Model` or `torch.nn.Module`, for more info about network definitions, check out [Tutorial 5](./t05_model.ipynb). The differentiable operations during training are listed as follows:\n",
    "\n",
    "1. Feed the preprocessed images to the network and get prediction scores.\n",
    "2. Calculate `CrossEntropy` (loss) between prediction scores and ground truth.\n",
    "3. Update the model by minimizing `CrossEntropy`.\n",
    "\n",
    "For more info about `Network` and its operators, check out [Tutorial 6](./t06_network.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 20:24:01.215903: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-01 20:24:03.646903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38401 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:07:00.0, compute capability: 8.0\n",
      "2023-11-01 20:24:03.648525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38401 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:0f:00.0, compute capability: 8.0\n",
      "2023-11-01 20:24:03.649917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38401 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:b7:00.0, compute capability: 8.0\n",
      "2023-11-01 20:24:03.651256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 38401 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:bd:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "from fastestimator.architecture.tensorflow import LeNet\n",
    "# from fastestimator.architecture.pytorch import LeNet  # One can also use a pytorch model\n",
    "\n",
    "from fastestimator.op.tensorop.loss import CrossEntropy\n",
    "from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n",
    "\n",
    "model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n",
    "\n",
    "network = fe.Network(ops=[\n",
    "        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n",
    "        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n",
    "        UpdateOp(model=model, loss_name=\"ce\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Estimator'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Estimator\n",
    "We define the `Estimator` to connect the `Network` to the `Pipeline`, and compute accuracy as a validation metric. Please see [Tutorial 7](./t07_estimator.ipynb) for more about `Estimator` and `Traces`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastestimator.trace.metric import Accuracy\n",
    "from fastestimator.trace.io import BestModelSaver\n",
    "import tempfile\n",
    "\n",
    "traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n",
    "          BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]\n",
    "\n",
    "estimator = fe.Estimator(pipeline=pipeline,\n",
    "                         network=network,\n",
    "                         epochs=2,\n",
    "                         traces=traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Training'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mFastEstimator-Warn: Expected PyTorch version 2.0.1 but found 2.0.0+cu118. The framework may not work as expected.\u001b[00m\n",
      "    ______           __  ______     __  _                 __            \n",
      "   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n",
      "  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n",
      " / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n",
      "/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n",
      "                                                                        \n",
      "\n",
      "FastEstimator-Start: step: 1; logging_interval: 100; num_device: 4;\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "FastEstimator-Train: step: 1; ce: 2.2958455;\n",
      "FastEstimator-Train: step: 100; ce: 0.22946885; steps/sec: 90.06;\n",
      "FastEstimator-Train: step: 200; ce: 0.16976535; steps/sec: 102.36;\n",
      "FastEstimator-Train: step: 300; ce: 0.25945413; steps/sec: 105.25;\n",
      "FastEstimator-Train: step: 400; ce: 0.23129767; steps/sec: 104.83;\n",
      "FastEstimator-Train: step: 500; ce: 0.06707792; steps/sec: 102.44;\n",
      "FastEstimator-Train: step: 600; ce: 0.049119912; steps/sec: 107.09;\n",
      "FastEstimator-Train: step: 700; ce: 0.029562747; steps/sec: 107.46;\n",
      "FastEstimator-Train: step: 800; ce: 0.016698593; steps/sec: 90.08;\n",
      "FastEstimator-Train: step: 900; ce: 0.0118493; steps/sec: 99.57;\n",
      "FastEstimator-Train: step: 1000; ce: 0.077762395; steps/sec: 107.74;\n",
      "FastEstimator-Train: step: 1100; ce: 0.1310657; steps/sec: 109.58;\n",
      "FastEstimator-Train: step: 1200; ce: 0.00404916; steps/sec: 105.33;\n",
      "FastEstimator-Train: step: 1300; ce: 0.052234273; steps/sec: 94.8;\n",
      "FastEstimator-Train: step: 1400; ce: 0.059791937; steps/sec: 109.59;\n",
      "FastEstimator-Train: step: 1500; ce: 0.13681616; steps/sec: 112.34;\n",
      "FastEstimator-Train: step: 1600; ce: 0.03644333; steps/sec: 112.15;\n",
      "FastEstimator-Train: step: 1700; ce: 0.004571152; steps/sec: 96.74;\n",
      "FastEstimator-Train: step: 1800; ce: 0.029358344; steps/sec: 110.72;\n",
      "FastEstimator-Train: step: 1875; epoch: 1; epoch_time(sec): 67.26;\n",
      "Eval Progress: 1/313;\n",
      "Eval Progress: 104/313; steps/sec: 106.4;\n",
      "Eval Progress: 208/313; steps/sec: 100.59;\n",
      "Eval Progress: 313/313; steps/sec: 76.71;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmp1p60ersk/model_best_accuracy.h5\n",
      "FastEstimator-Eval: step: 1875; epoch: 1; accuracy: 0.9833; ce: 0.05148898; max_accuracy: 0.9833; since_best_accuracy: 0;\n",
      "FastEstimator-Train: step: 1900; ce: 0.08898077; steps/sec: 1.96;\n",
      "FastEstimator-Train: step: 2000; ce: 0.09282805; steps/sec: 92.7;\n",
      "FastEstimator-Train: step: 2100; ce: 0.17321953; steps/sec: 89.75;\n",
      "FastEstimator-Train: step: 2200; ce: 0.004797754; steps/sec: 92.91;\n",
      "FastEstimator-Train: step: 2300; ce: 0.03945522; steps/sec: 102.26;\n",
      "FastEstimator-Train: step: 2400; ce: 0.121083125; steps/sec: 88.34;\n",
      "FastEstimator-Train: step: 2500; ce: 0.019869003; steps/sec: 99.29;\n",
      "FastEstimator-Train: step: 2600; ce: 0.030041546; steps/sec: 97.76;\n",
      "FastEstimator-Train: step: 2700; ce: 0.2057406; steps/sec: 97.23;\n",
      "FastEstimator-Train: step: 2800; ce: 0.0117459055; steps/sec: 100.15;\n",
      "FastEstimator-Train: step: 2900; ce: 0.0042889104; steps/sec: 92.81;\n",
      "FastEstimator-Train: step: 3000; ce: 0.060711075; steps/sec: 99.56;\n",
      "FastEstimator-Train: step: 3100; ce: 0.045057233; steps/sec: 89.32;\n",
      "FastEstimator-Train: step: 3200; ce: 0.14784154; steps/sec: 95.03;\n",
      "FastEstimator-Train: step: 3300; ce: 0.05716361; steps/sec: 103.68;\n",
      "FastEstimator-Train: step: 3400; ce: 0.02522368; steps/sec: 87.46;\n",
      "FastEstimator-Train: step: 3500; ce: 0.008144757; steps/sec: 99.97;\n",
      "FastEstimator-Train: step: 3600; ce: 0.0045928275; steps/sec: 97.52;\n",
      "FastEstimator-Train: step: 3700; ce: 0.053977456; steps/sec: 100.66;\n",
      "FastEstimator-Train: step: 3750; epoch: 2; epoch_time(sec): 69.67;\n",
      "Eval Progress: 1/313;\n",
      "Eval Progress: 104/313; steps/sec: 112.96;\n",
      "Eval Progress: 208/313; steps/sec: 107.19;\n",
      "Eval Progress: 313/313; steps/sec: 92.28;\n",
      "FastEstimator-BestModelSaver: Saved model to /tmp/tmp1p60ersk/model_best_accuracy.h5\n",
      "FastEstimator-Eval: step: 3750; epoch: 2; accuracy: 0.9864; ce: 0.043881364; max_accuracy: 0.9864; since_best_accuracy: 0;\n",
      "FastEstimator-Finish: step: 3750; model_lr: 0.001; total_time(sec): 238.9;\n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Inferencing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing\n",
    "After training, we can do inferencing on new data with `Pipeline.transform` and `Netowork.transform`. Please checkout [Tutorial 8](./t08_mode.ipynb) for more details. \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "Ground truth class is 7\n",
      "Predicted class is 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAYAAACAvzbMAAAbe0lEQVR4Xu3df8zvZV3H8c+hhH8gcDXBRcxCiIgkhk1p+KM2PIBr0JwkycrQCUGJOEUrFywEaSQO0xLGsZbMOWgDZ238CHIlLA2C4BwtgYDGWRMqko5FwfG0G3XLHweu+7qvz+f7uj6fh3+19b6uz/t6vt73/dz7Pss27dq1a9fgPwgggAACCKyTwCYCWScx5QgggAACzxAgEIOAAAIIIFBFgECqsDmEAAIIIEAgZgABBBBAoIoAgVRhcwgBBBBAgEDMAAIIIIBAFQECqcLmEAIIIIAAgZgBBBBAAIEqAgRShc0hBBBAAAECMQMIIIAAAlUECKQKm0MIIIAAAgRiBhBAAAEEqggQSBU2hxBAAAEECMQMIIAAAghUESCQKmwOIYAAAggQiBlAAAEEEKgiQCBV2BxCAAEEECAQM4AAAgggUEWAQKqwOYQAAgggQCBmAAEEEECgigCBVGFzCAEEEECAQMwAAggggEAVAQKpwuYQAggggACBmAEEEEAAgSoCBFKFzSEEEEAAAQIxAwgggAACVQQIpAqbQwgggAACBGIGEEAAAQSqCBBIFTaHEEAAAQQIxAwggAACCFQRIJAqbA4hgAACCBCIGUAAAQQQqCJAIFXYHEIAAQQQIBAzgAACCCBQRYBAqrA5hAACCCBAIGYAAQQQQKCKAIFUYXMIAQQQQIBAzAACCCCAQBUBAqnC5hACCCCAAIGYAQQQQACBKgIEUoXNoTkSuPQPPjn88TU3DBe95y3Dyccf+y1PvOoTfz588Mprh3Pe8rrhraf93Byf700IrJsAgawbmQNzJfDU0zuHN5594fDwI18erv/Y+4YX7v/9zzz1vgcfGV7/1guGo4548bDlA+8e9thj01wReBcC6yJAIOvCpXjuBP55+5eH173lt4efOOxHhi2XnTfs/NrXhjec+TvDvzz6b8N1W943vOAH9ps7Au9DoJgAgRSjUrgUAtff8Nnhty65aviNX3/j8B9f2TH84Z98avjwxecMP/PTRy0FgXciUESAQIowKVoagfMu/OjwF3995/D0zp3DG0762eE333ba0hB4LwLPSYBAnhORgiUSuGvrfcNpv3bRM0//1B9dNLz4h39wiRi8GYFnJUAgBgSBbyPwP//71HDKGRcMT/znV4e1//mHXviC4eqPvHd43vd+D1YIIPD/CBCIcUDg2whcdPnHh09cd8uw5QPnDU/s+K/h3PM/PLz51BOHd5xxClYIIEAgZgCB707gr/7m74dffc8Hh196/ebh3Wef+kzRey6+cvj0Tbc/I5SXH304dAgg8A0CNhCjgMA3CPzrv39l+PnT3zs8f7/vG6698oJhrz2f98z/ZsdX/3s4+fT3Dk8/vXO47mMXDs/fdx/MEEBgGAYCMQYIDMOwa9eu4cx3XzZ87u++MHzyo+cPh734oG/h8rd3/8PwK+f+7vCqY44cPnLx2zFDAAECMQMIIIAAArUEbCC15JxDAAEEFk6AQBY+AJ6PAAII1BIgkFpyziGAAAILJ0AgCx8Az0cAAQRqCRBILTnnEEAAgYUTIJCFD4DnI4AAArUECKSWnHMIIIDAwgnMRiCbNvn/ErfwWfZ8BLohsPZ/uDqH/xDIHFL0BgQQ6IoAgYTFZQMJC0Q7CCCwWwIEEjYcBBIWiHYQQIBAepkBAuklKX0igIANJGwGCCQsEO0ggIANpJcZIJBektInAgjYQMJmgEDCAtEOAgjYQHqZAQLpJSl9IoCADSRsBggkLBDtIICADaSXGSCQXpLSJwII2EDCZoBAwgLRDgII2EB6mQEC6SUpfSKAgA0kbAYIJCwQ7SCAgA2klxkgkF6S0icCCNhAwmaAQMIC0Q4CCNhAepkBAuklKX0igIANJGwGCCQsEO0ggIANpJcZIJBektInAgjYQMJmgEDCAtEOAgjYQHqZAQLpJSl9IoCADSRsBggkLBDtIICADaSXGSCQXpLSJwII2EDCZoBAwgLRDgII2EB6mQEC6SUpfSKAgA0kbAYIJCwQ7SCAgA2klxkgkF6S0icCCNhAwmaAQMIC0Q4CCNhAepkBAuklKX0igIANJGwGCCQsEO0ggIANpJcZIJBektInAgjYQMJmgEDCAtEOAgjYQHqZAQLpJSl9IoCADSRsBggkLBDtIICADaSXGSCQXpLSJwII2EDCZoBAwgLRDgII2EB6mQEC6SUpfSKAgA0kbAYIJCwQ7SCAgA2klxkgkF6S0icCCNhAwmaAQMIC0Q4CCNhAepkBAuklKX0igIANJGwGCCQsEO0ggIANpJcZIJBektInAgjYQMJmgEDCAtEOAgjYQHqZAQLpJSl9IoCADSRsBggkLBDtIICADaSXGSCQXpLSJwII2EDCZoBAwgLRDgII2EB6mQEC6SUpfSKAgA0kbAYIJCwQ7SCAgA2klxkgkF6S0icCCNhAwmaAQMIC0Q4CCNhAepkBAuklKX0igIANJGwGCCQsEO0ggIANpJcZIJBektInAgjYQMJmgEDCAtEOAgjYQHqZAQLpJSl9IoCADSRsBggkLBDtIICADaSXGSCQXpLSJwII2EDCZoBAwgLRDgII2EB6mQEC6SUpfSKAgA0kbAYIJCwQ7SCAgA2klxkgkF6S0icCCNhAwmaAQMIC0Q4CCNhAepkBAuklKX0igIANJGwGCCQsEO0ggIANpJcZIJBektInAgjYQMJmgEDCAtEOAgjYQHqZAQLpJSl9IoCADSRsBggkLBDtIICADaSXGSCQXpLSJwII2EDCZoBAwgLRDgII2EB6mQEC6SUpfSKAgA0kbAYIJCwQ7SCAgA2klxkgkF6S0icCCNhAwmZg6QI54fjjixM5/c2nF9c+8sgjRbVPPvlkUd1a0bXX/mlx7aOPPlpcW9pr8YUKERiJAIGMBLb2WgIhEAKp/elxbmoCBDI18ef4HoEQCIGE/VBqx7+B9DIDBEIgBNLLT6s+bSBhM0AgBEIgYT+U2rGB9DIDBEIgBNLLT6s+bSBhM0AgBEIgYT+U2rGB9DIDBEIgBNLLT6s+bSBhM0AgBEIgYT+U2rGB9DIDBEIgBNLLT6s+bSBhM0AgBEIgYT+U2rGB9DIDSxfItm33Fkd12GE/Vly76sLHH3+8uIU777yzuFZhGYGHH364rHAYhssv/1Bx7bZt24pr51hoAwlLlUAIhEDa/1ASSHumazcSyDhcq28lEAIhkOofn90eJJD2TAlkHKYbupVACIRANvQj9F0PE0h7pgQyDtMN3UogBEIgG/oRIpD2+HZ7oz9hTQi75FMEQiAEUvKTsr4aG8j6eJVWE0gpqYnqCIRACKT9DxuBtGfqT1jjMN3QrQRCIASyoR8hf8Jqj8+fsCZkuqFPEQiBEMiGfoQIpD0+ApmQ6YY+RSAEQiAb+hEikPb4CGRCphv61NIFcswxxxTz+/HDDy+u/ccvfamo9kcPPbSobq3oyJ88srh28+bNxbUHH3xwUe19991XVLdWdMghhxTXjlH41FNPFV+7ffv24toXvehFxbWlhRecf35p6XDh+y4qrp1joX9ED0uVQAiEQAgk7NeSDaSXQAiEQAiEQHr5fWUDCUuKQAiEQAgk7NeSDaSXQAiEQAiEQHr5fWUDCUuKQAiEQAgk7NeSDaSXQAiEQAiEQHr5fWUDCUuKQAiEQAgk7NeSDaSXQAiEQAiEQHr5fWUDCUuKQAiEQAgk7NeSDaSXQAiEQAiEQHr5fWUDCUtq6QIJi6NZO3vvvXfxXUcccURR7T333FNUt1Z05JHl/7UrxZeuo/DJJ58srn7ggQeKa7/4xS8U1R5wwAFFdWtFb/rlNxXXfvzqq4tr51hIIGGpEkhYII3aIRACaTRKUdcQSFQcw0AgYYE0aodACKTRKEVdQyBRcRBIWBzN2iEQAmk2TEEXEUhQGGut2EDCAmnUDoEQSKNRirqGQKLiIJCwOJq1QyAE0myYgi4ikKAwbCBhYTRsh0AIpOE4xVxFIDFRfL0Rf8IKC6RROwRCII1GKeoaAomKg0DC4mjWDoEQSLNhCrqIQILCsIGEhdGwHQIhkIbjFHMVgcRE4U9YYVE0bYdACKTpQIVcRiAhQXyzDf8GEhaIdiYncPzmzcXf/PSffbqo9vOf/1xR3VrR5s0nFNfu2LGjuHaOhQQSliqBhAWinckJEMjkyKs/SCDV6MY5SCDjcHVrPwQIpJ+sCCQsKwIJC0Q7kxMgkMmRV3+QQKrRjXOQQMbh6tZ+CBBIP1kRSFhWBBIWiHYmJ0AgkyOv/iCBVKMb5yCBjMPVrf0QIJB+siKQsKwIJCwQ7UxOgEAmR179QQKpRjfOQQIZh6tb+yFAIP1kRSBhWRFIWCDamZwAgUyOvPqDBFKNbpyDBDIOV7f2Q4BA+smKQMKyIpCwQLTThMC+++5bfM+9995TXHvggQcW1Z54wolFdWtFN9x4Y3Ht0gsJJGwCCCQsEO00IUAgTTDGXUIgYZEQSFgg2mlCgECaYIy7hEDCIiGQsEC004QAgTTBGHcJgYRFQiBhgWinCQECaYIx7hICCYuEQMIC0U4TAgTSBGPcJQQSFgmBhAWinSYECKQJxrhLCCQsEgIJC0Q7TQgQSBOMcZcQSFgkBBIWiHaaECCQJhjjLiGQsEgIJCwQ7TQhQCBNMMZdQiBhkRBIWCDaaULgHeeeW3zPpb93aXHtY489VlR77LGvKKpbK7r//vuLa5deSCBhE0AgYYFopwkBAmmCMe4SAgmLhEDCAtFOEwIE0gRj3CUEEhYJgYQFop0mBAikCca4SwgkLBICCQtEO00IEEgTjHGXEEhYJAQSFoh2mhAgkCYY4y4hkLBICCQsEO00IUAgTTDGXUIgYZEQSFgg2mlCgECaYIy7hEDCIiGQsEC004QAgTTBGHcJgYRFQiBhgWinCQECaYIx7hICCYuEQMIC0U4TAgTSBGPcJQQSFgmBhAWind0SOOqoo4rp3HbbZ4tr99prr+Lal73s5UW1d9xxR1GdovURIJD18Rq9mkBGR+wDjQgQSCOQHV9DIGHhEUhYINqxgZiB3RIgkLDhIJCwQLRDIGaAQHqZAQLpJSl9+hOWGbCBhM0AgYQFoh0biBmwgfQyAwTSS1L6tIGYARtI2AwQSFgg2rGBmAEbSC8zQCC9JKVPG4gZsIGEzQCBhAWiHRuIGbCB9DIDBNJLUvq0gZgBG0jYDBBIWCDa2S2B8971zmI677/kkuLa66+/vrj2lFN+oah2586dRXWK1keAQNbHa/RqAhkdsQ80IkAgjUB2fA2BhIVHIGGBaMcGYgb8G0gvM0AgvSSlTxuIGbCBhM0AgYQFoh0biBmwgfQyAwTSS1L6tIGYARtI2AwQSFgg2rGBmAEbSC8zQCC9JKVPG4gZsIGEzQCBhAWiHRuIGbCB9DIDBNJLUvq0gZgBG0jYDBBIWCALbGfPPfcsevXNN99YVLdW9NKX/lRx7bHHvqK49q677iquVdieAIG0Z7qhGwlkQ/gcbkCAQBpAXMgVBBIWNIGEBbLAdghkgaFXPplAKsGNdYxAxiLr3lICBFJKSh2BhM0AgYQFssB2CGSBoVc+mUAqwY11jEDGIuveUgIEUkpKHYGEzQCBhAWywHYIZIGhVz6ZQCrBjXWMQMYi695SAgRSSkodgYTNAIGEBbLAdghkgaFXPplAKsGNdYxAxiLr3lICBFJKSh2BhM0AgYQFssB2CGSBoVc+mUAqwY11jEDGIuveUgJvP+dtRaUfuOyyorq1omuuuaa49tRTf7G4VuFqCRDIavl/x9cJJCyQBbZDIAsMvfLJBFIJbqxjBDIWWfeWEiCQUlLqCCRsBggkLJAFtkMgCwy98skEUglurGMEMhZZ95YSIJBSUuoIJGwGCCQskAW2QyALDL3yyQRSCW6sYwQyFln3lhIgkFJS6ggkbAYIJCyQBbZDIAsMvfLJBFIJbqxjBDIWWfeWEiCQUlLqCCRsBggkLJAFtkMgCwy98skEUglurGMEMhZZ95YSIJBSUuoIJGwGCCQskJm08+pXv7r4JTfeeENR7RNPPFFUt1Z03HGvKa69++67i2sVrpYAgayW/3d8nUDCAplJOwQykyDDnkEgYYEQSFggM2mHQGYSZNgzCCQsEAIJC2Qm7RDITIIMewaBhAVCIGGBzKQdAplJkGHPIJCwQAgkLJCZtEMgMwky7BkEEhYIgYQFMpN2CGQmQYY9g0DCAiGQsEBm0g6BzCTIsGcQSFggBBIWyEzaIZCZBBn2DAIJC4RAwgKZSTsEMpMgw55BIGGBEEhYIDNph0BmEmTYMwgkLBACCQskuJ199tmnuLvbb7+tuPbwww8vqr3iiiuK6taKzjrr7OJahf0QIJCwrAgkLJDgdggkOJyFtEYgYUETSFggwe0QSHA4C2mNQMKCJpCwQILbIZDgcBbSGoGEBU0gYYEEt0MgweEspDUCCQuaQMICCW6HQILDWUhrBBIWNIGEBRLcDoEEh7OQ1ggkLGgCCQskuB0CCQ5nIa0RSFjQBBIWSHA7BBIczkJaI5CwoAkkLJDgdggkOJyFtEYgYUETSFggK2hnjz32KPrqLbfcXFS3VvTKV76quHbr1q1FtSee+NqiurWi7du3F9cq7IcAgYRlRSBhgaygHQJZAXSfrCJAIFXYxjtEIOOx7eVmAuklKX0SSNgMEEhYICtoh0BWAN0nqwgQSBW28Q4RyHhse7mZQHpJSp8EEjYDBBIWyAraIZAVQPfJKgIEUoVtvEMEMh7bXm4mkF6S0ieBhM0AgYQFsoJ2CGQF0H2yigCBVGEb7xCBjMe2l5sJpJek9EkgYTNAIGGBrKAdAlkBdJ+sIkAgVdjGO0Qg47Ht5WYC6SUpfRJI2AwQSFggK2jnoIMOKvrqgw/+U1HdeouOO+41RUduvfXWojpF8yVAIGHZEkhYICtoh0BWAN0nqwgQSBW28Q4RyHhse7mZQHpJSp8EEjYDBBIWyAraIZAVQPfJKgIEUoVtvEMEMh7bXm4mkF6S0ieBhM0AgYQFsoJ2CGQF0H2yigCBVGEb7xCBjMe2l5sJpJek9EkgYTNAIGGBrKAdAlkBdJ+sIkAgVdjGO0Qg47Ht5WYC6SUpfRJI2AwQSFggK2iHQFYA3SerCBBIFbbxDhHIeGx7uZlAeklKnwQSNgMEEhZIo3b233//4ps+85m/LKo99NBDi+rWis4848zi2qu2bCmqncsvj6LHKvquBOYyA5t2zeQlBDLPn1QCmWeuS3/VTH7tDgSy9EkOfz+BhAekvSoCBFKFbbxDNpDx2K7yZgJZJX3fHosAgYxFtvJeAqkEF36MQMID0l4VAQKpwjbeIQIZj+0qbyaQVdL37bEIEMhYZCvvJZBKcOHHCCQ8IO1VESCQKmzjHSKQ8diu8mYCWSV93x6LAIGMRbbyXgKpBBd+jEDCA9JeFQECqcI23iECGY/tKm8mkFXS9+2xCBDIWGQr7yWQSnDhx8571zuLO3z/JZcU15YWvuQlR5aWDtu2bSuuVbhsAgQSlj+BhAXSqB0CaQTSNVEECCQqjmEgkLBAGrVDII1AuiaKAIFExUEgYXE0a4dAmqF0URABAgkKY60VG0hYII3aIZBGIF0TRYBAouIgkLA4mrVDIM1QuiiIAIEEhWEDCQujYTsE0hCmq2IIEEhMFF9vxJ+wwgJp1A6BNALpmigCBBIVB4GExdGsHQJphtJFQQQIJCgMG0hYGA3bIZCGMF0VQ4BAYqLwJ6ywKJq2QyBNcboshACBhATxzTb8G0hYIM/SztFHH13c7E033Vhcu99++xXXlhb6rzIpJaVuPQQIZD20JqglkAkgN/oEgTQC6ZpuCRBIWHQEEhaIDaSfQHQ6OQECmRz5s3+QQMICIZB+AtHp5AQIZHLkBBKGvLodf8KqRufgTAgQSFiQNpCwQGwg/QSi08kJEMjkyG0gYcir27GBVKNzcCYECCQsSBtIWCA2kH4C0enkBAhkcuQ2kDDk1e3YQKrROTgTAgQSFqQNJCwQG0g/geh0cgIEMjlyG0gY8up2bCDV6BycCQECCQvSBhIWyLO0c/ZZZxU3+6Hf/1BxbWnh1q1bS0uHk046ubj2oYceKq5VuGwCBBKWP4GEBUIg/QSi08kJEMjkyP0JKwx5dTs2kGp0Ds6EAIGEBWkDCQvEBtJPIDqdnACBTI7cBhKGvLodG0g1OgdnQoBAwoK0gYQFYgPpJxCdTk6AQCZHbgMJQ17djg2kGp2DMyFAIGFB2kDCArGB9BOITicnQCCTI7eBhCGvbscGUo3OwZkQIJCwIG0gYYHYQPoJRKeTEyCQyZHbQMKQV7djA6lG5+BMCBBIWJA2kLBAVrCB3H77bUUQTjjhtUV1a0U7duworlWIQCkBAiklNVEdgUwEusFnxtpACKRBOK6YhACBTIK5/CMEUs5q1ZUEsuoEfH/VBAhk1Ql82/cJJCwQf8LqJxCdTk6AQCZH/uwfJJCwQAikn0B0OjkBApkcOYGEIa9ux5+wqtE5OBMCBBIWpA0kLBAbSD+B6HRyAgQyOXIbSBjy6nZsINXoHJwJAQIJC9IGEhaIDaSfQHQ6OQECmRy5DSQMeXU7NpBqdA7OhACBhAVpAwkLRDsIILBbAgQSNhwEEhaIdhBAgEB6mQEC6SUpfSKAgA0kbAYIJCwQ7SCAgA2klxkgkF6S0icCCNhAwmaAQMIC0Q4CCNhAepkBAuklKX0igIANJGwGCCQsEO0ggIANpJcZIJBektInAgjYQMJmgEDCAtEOAgjYQHqZAQLpJSl9IoCADcQMIIAAAggsmsCmXXNR4aJj9HgEEEBgegIEMj1zX0QAAQRmQYBAZhGjRyCAAALTEyCQ6Zn7IgIIIDALAgQyixg9AgEEEJieAIFMz9wXEUAAgVkQIJBZxOgRCCCAwPQECGR65r6IAAIIzIIAgcwiRo9AAAEEpidAINMz90UEEEBgFgQIZBYxegQCCCAwPQECmZ65LyKAAAKzIEAgs4jRIxBAAIHpCRDI9Mx9EQEEEJgFAQKZRYwegQACCExPgECmZ+6LCCCAwCwIEMgsYvQIBBBAYHoCBDI9c19EAAEEZkGAQGYRo0cggAAC0xMgkOmZ+yICCCAwCwIEMosYPQIBBBCYngCBTM/cFxFAAIFZECCQWcToEQgggMD0BAhkeua+iAACCMyCAIHMIkaPQAABBKYnQCDTM/dFBBBAYBYECGQWMXoEAgggMD0BApmeuS8igAACsyBAILOI0SMQQACB6QkQyPTMfREBBBCYBQECmUWMHoEAAghMT4BApmfuiwgggMAsCBDILGL0CAQQQGB6AgQyPXNfRAABBGZBgEBmEaNHIIAAAtMTIJDpmfsiAgggMAsCBDKLGD0CAQQQmJ4AgUzP3BcRQACBWRAgkFnE6BEIIIDA9AT+D/OT7lAflubqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = pipeline.get_results(mode=\"eval\")\n",
    "data = network.transform(data, mode=\"eval\")\n",
    "predictions = np.argmax(data[\"y_pred\"], axis=-1)\n",
    "\n",
    "print(\"Ground truth class is {}\".format(data[\"y\"][0]))\n",
    "print(\"Predicted class is {}\".format(predictions[0]))\n",
    "img = fe.util.BatchDisplay(title=\"x\", image=data[\"x\"][:1])\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t01Apphub'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apphub Examples\n",
    "You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:\n",
    "\n",
    "* [MNIST](../../apphub/image_classification/mnist/mnist.ipynb)\n",
    "* [DNN](../../apphub/tabular/dnn/dnn.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
